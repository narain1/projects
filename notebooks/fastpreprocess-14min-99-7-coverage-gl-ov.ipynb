{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VFGUfg7XPz_d"
   },
   "source": [
    "# Super fast pre-processing and modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbARwLAkQWQb"
   },
   "source": [
    "##Pre-processing\n",
    "\n",
    "After our EDA: https://www.kaggle.com/s7anmerk/lean-import-to-save-ram-and-eda\n",
    "\n",
    "In the following part we will prepare the data for using it in various models. Therefore, we wrote different functions which we step by step improved during the learning phase. For getting more information how we got towards the functions and dictionaries, please check out the 'Compare embeddings' file.\n",
    "\n",
    "1. Load datasets\n",
    "2. Set up the methods to be used in the preprocessing\n",
    "3. **Prepprocessing for Neural Networks using word embeddings (14minutes, only improves unknown words! So no deletion of known embedding words in the text)**\n",
    "4. Testing !!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXMaNc9pPp81"
   },
   "source": [
    "### Load libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jK7ZdLsIfI-1"
   },
   "outputs": [],
   "source": [
    "#%reset\n",
    "#!pip install pyspellchecker\n",
    "#!pip install googletrans\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import re\n",
    "import numpy as np\n",
    "#from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from googletrans import Translator\n",
    "from textblob import Word\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "import gc\n",
    "gc.enable()\n",
    "!pip install emoji\n",
    "import emoji\n",
    "from nltk.corpus import wordnet\n",
    "import datetime\n",
    "import time\n",
    "import operator\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm, trange\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZrd-QwbR8h1"
   },
   "source": [
    "### Used methods within the pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mybhfy9Ia0Ry"
   },
   "source": [
    "We wrote different methods to pre-process the data. These methods are design so that they can e used for different kind of models using different vectorizer types.\n",
    "The methods are optimized by using FastText and Glove Embedding Indexes.\n",
    "\n",
    "**Our method includes functions like:**\n",
    "  -** Replace contractions** by its long version such as *'I'm' --> 'I am'* or *'Let's' 'Let us'*\n",
    "  - **Replace special symbols** like dashes, underscores and slashes which are typically used to separate words. \n",
    "    It can keep words like dashes in words like* 'e-mail'* known to the embedding index while removing it for '*anti-Trump*' where the index has no entry\n",
    "  - **Replace** typical keyboard typed **smilies** into its simple translation such as *'-)' --> 'smile'*\n",
    "  - **Replace stopwords**\n",
    "  - **Clean the text from unknown characters** by either translating it or by removing it\n",
    "  - **Transform standalone numbers** for example *'5337' to '####'* to reduce noise\n",
    "  - **Replace year and hour** abbreviations to the full word\n",
    "  - TextBlob **lemmatizer**, which we found to work relatively stable compared to others\n",
    "  \n",
    "**Further methods are used to handle and compare the datasets and embeddings**\n",
    "  - **Build a vocabulary** from a dataset together with the word frequency\n",
    "  - **Load different embeddings and build a vocabulary** from GloVe, FastText or GoogleNews in its specific ways\n",
    "  - **Check the coverage** of embedding indexes against the dataset vocabulary, returning unknown words\n",
    "  \n",
    "  \n",
    "  \n",
    "  All these methods are combined in different ways to serve the specific requirements of a vectorizer and model combination.\n",
    "  This will be explained in detail for each pre-processing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgz0T-SKfW7U"
   },
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "  \n",
    "  \"\"\"\n",
    "  This functions check's whether a text contains contractions or not. \n",
    "  In case a contraction is found, the corrected value from the dictionary is \n",
    "  returned.\n",
    "  Example: \"I've\" towards \"I have\"\n",
    "  \"\"\"\n",
    "  \n",
    "  #replace words with \"'ve\" to \"have\"\n",
    "  matches = re.findall(r'\\b\\w+[\\'`´]ve\\b', text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r'[\\'`´]ve\\b', \" have\", text)\n",
    "  \n",
    "  #replace words with \"'re\" to \"are\"\n",
    "  matches = re.findall(r'\\b\\w+[\\'`´]re\\b', text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r'[\\'`´]re\\b', \" are\", text)\n",
    "  \n",
    "  #replace words with \"'ll\" to \"will\"\n",
    "  matches = re.findall(r'\\b\\w+[\\'`´]ll\\b', text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r'[\\'`´]ll\\b', \" will\", text)\n",
    "  \n",
    "  #replace words with \"'m\" to \"am\"\n",
    "  matches = re.findall(r'\\b\\w+[\\'`´]m\\b', text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r'[\\'`´]m\\b', \" am\", text)\n",
    "  \n",
    "  #replace words with \"'d\" to \"would\"\n",
    "  matches = re.findall(r'\\b\\w+[\\'`´]d\\b', text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r'[\\'`´]d\\b', \" would\", text)\n",
    "  \n",
    "  #replace words with contraction according to the contraction_dict\n",
    "  matches = re.findall(r'\\b\\w+[\\'`´]\\w+\\b', text)\n",
    "  for x in matches:\n",
    "    if x in contraction_dict.keys():\n",
    "      text = text.replace(x, contraction_dict.get(x))\n",
    "  \n",
    "  # replace all \"'s\" by space\n",
    "  matches = re.findall(r'\\b\\w+[\\'`´]s\\b', text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r'[\\'`´]s\\b', \" \", text)\n",
    "  return text\n",
    "\n",
    "# Dictionary of contractions coming out of the pre-investigation in the other kernel\n",
    "contraction_dict = {\"Can't\":\"can not\", \"Didn't\":\"did not\", \"Doesn't\":\"does not\", \n",
    "                    \"Isn't\":\"is not\", \"Don't\":\"do not\", \"Aren't\":\"are not\", \"#\":\"\",\n",
    "                    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\",\n",
    "                    \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                    \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
    "                    \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
    "                    \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\",\n",
    "                    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n",
    "                    \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\",\n",
    "                    \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n",
    "                    \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                    \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "                    \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
    "                    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
    "                    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                    \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
    "                    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
    "                    \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n",
    "                    \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "                    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "                    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "                    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "                    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "                    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "                    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
    "                    \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                    \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                    \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\n",
    "                    \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                    \"you're\": \"you are\", \"you've\": \"you have\", \"c'mon\":\"come on\",\n",
    "                    \"Don''t\":\"do not\", \"Haden't\":\"had not\", \"Grab'em\":\"grab them\", \"USA''s\":\"USA\",\n",
    "                    \"Pick'em\":\"pick them\", \"I'lll\":\"I will\", \"Tell'em\":\"tell them\", \"Y'all\":\"you all\",\n",
    "                    \"Wouldn't\":\"would not\", \"Shouldn't\":\"should not\", \"I'DVE\":\"I would have\",\n",
    "                    \"SHOOT'UM\":\"shoot them\", \"CANN'T\":\"can not\", \"COUD'VE\":\"could have\", \"Yo'ure\":\"you are\",\n",
    "                    \"LOCK'EM\":\"lock them\", \"G'night\":\"goodnight\", \"W'ell\":\"we will\", \"IT'D\":\"it would\",\n",
    "                    \"Couldn't\":\"could not\", \"LOCK'UM\":\"lock them\", \"WOULD'NT\":\"would not\", \"Cant't\":\"can not\",\n",
    "                    \"HADN'T\":\"had not\", \"It''s\":\"it is\", \"Don'ts\":\"do not\", \"Arn't\":\"are not\",\n",
    "                    \"We'll\":\"we will\", \"G'Night\":\"goodnight\", \"THAT'LL\":\"that will\", \"Dpn't\":\"do not\",\n",
    "                    \"Idon'tgetitatall\":\"I do not get it at all\", \"THEY'VE\":\"they have\", \"Le'ts\":\"let us\",\n",
    "                    \"SEND'EM\":\"send them\", \"AIN'T\":\"is not\", \"WE'D\":\"we would\", \"I'vemade\":\"I have made\",\n",
    "                    \"SHE'LL\":\"she will\", \"I'llbe\":\"I will be\", \"I'mma\":\"I am a\", \"Could'nt\":\"could not\",\n",
    "                    \"You'very\":\"you are very\", \"Light'em\":\"light them\", \"Con't\":\"can not\", \"I'Μ\":\"I am\",\n",
    "                    \"Kick'em\":\"kick them\", \"Shoudn't\":\"should not\", \"That''s\":\"that is\",\n",
    "                    \"Didn't_work\":\"did not work\", \"You'rethinking\":\"you are thinking\", \"Dn't\":\"do not\",\n",
    "                    \"CON'T\":\"can not\", \"DON'T\":\"do not\", \"C'Mon\":\"come on\", \"You'res\":\"you are\",\n",
    "                    \"Amn't\":\"is not\", \"WE'RE\":\"we are\", \"Can't\":\"can not\", \"Kouldn't\":\"could not\",\n",
    "                    \"SHouldn't\":\"should not\", \"Does't\":\"does not\", \"COULD'VE\":\"could have\",\n",
    "                    \"TrumpIDin'tCare\":\"Trump did not care\", \"Iv'e\":\"I have\", \"Dose't\":\"does not\",\n",
    "                    \"DOESEN'T\":\"does not\", \"Give'em\":\"give them\", \"Won'tdo\":\"will not do\",\n",
    "                    \"They'l\":\"they will\", \"He''s\":\"he is\", \"I'veve\":\"I have\", \"Wern't\":\"were not\",\n",
    "                    \"Pay'um\":\"pay them\", \"She''l\":\"she will\", \"Y'know\":\"you know\", \"DIdn't\":\"did not\",\n",
    "                    \"O'bamacare\":\"Obamacare\", \"I'ma\":\"I am a\", \"Ma'am\":\"madam\", \"WASN'T\":\"was not\",\n",
    "                    \"Dont't\":\"do not\", \"Is't\":\"is not\", \"OU'RE\":\"you are\", \"YOU'RE\":\"you are\",\n",
    "                    \"Ther'es\":\"there is\", \"C'mooooooon\":\"come on\", \"They_didn't\":\"they did not\",\n",
    "                    \"Som'thin\":\"something\", \"Love'em\":\"love them\", \"You''re\":\"you are\", \"I'D\":\"I would\",\n",
    "                    \"HASN'T\":\"has not\", \"WOULD'VE\":\"would have\", \"WAsn't\":\"was not\", \"ARE'NT\":\"are not\",\n",
    "                    \"Dowsn't\":\"does not\", \"It'also\":\"it is also\", \"Geev'um\":\"give them\", \"Theyv'e\":\"they have\",\n",
    "                    \"Theyr'e\":\"they are\", \"Take'em\":\"take them\", \"Book'em\":\"book them\", \"Havn't\":\"have not\",\n",
    "                    \"DOES'NT\":\"does not\", \"Who''s\":\"who is\", \"WON't\":\"will not\", \"I'Il\":\"I will\",\n",
    "                    \"I'don\":\"I do not\", \"AREN'T\":\"are not\", \"Ev'rybody\":\"everybody\", \"Hold'um\":\"hold them\",\n",
    "                    \"WE'LL\":\"we will\", \"Cab't\":\"can not\", \"IJustDon'tThink\":\"I just do not think\",\n",
    "                    \"Wouldn'T\":\"would not\", \"U'r\":\"you are\", \"I''ve\":\"I have\", \"DONT'T\":\"do not\",\n",
    "                    \"G'morning\":\"good morning\", \"You'ld\":\"you would\", \"We''ll\":\"we will\", \"YOUR'E\":\"you are\",\n",
    "                    \"TrumpDoesn'tCare\":\"Trump does not care\", \"Wasn't\":\"was not\", \"You'all\":\"you all\",\n",
    "                    \"Y'ALL\":\"you all\", \"G'bye\":\"goodbye\", \"YOU'VE\":\"you have\", \"Does'nt\":\"does not\",\n",
    "                    \"Don'TCare\":\"do not care\",  \"Weren't\":\"were not\", \"Y'All\":\"you all\", \"They'lll\":\"they will\",\n",
    "                    \"You'reOnYourOwnCare\":\"you are on your own care\", \"I'veposted\":\"I have posted\",\n",
    "                    \"Run'em\":\"run them\", \"Vote'em\":\"vote them\", \"Would't\":\"would not\", \"I'l\":\"I will\",\n",
    "                    \"Ddn't\":\"did not\", \"I'mm\":\"I am\", \"Sshouldn't\":\"should not\", \"Your'e\":\"you are\",\n",
    "                    \"I'v\":\"I have\", \"We'really\":\"we are really\", \"DOESN'T\":\"does not\", \"DiDn't\":\"did not\",\n",
    "                    \"Needn't\":\"need not\", \"They'er\":\"they are\", \"Look'em\":\"look them\", \"I'vÈ\":\"I have\",\n",
    "                    \"Didn`t\":\"did not\", \"I'lll\":\"I will\", \"Wouldn't\":\"would not\", \"It`s\":\"it is\", \"What's\":\"what is\",\n",
    "                    \"ISN`T\":\"is not\", \"WE'RE\":\"we are\", \"Are'nt\":\"are not\", \"DOesn't\":\"does not\", \"I'M\":\"I am\",\n",
    "                    \"WON'T\":\"will not\", \"WEREN'T\":\"were not\", \"TrumpDon'tCareAct\":\"Trump do not care act\",\n",
    "                    \"HAVEN'T\":\"have not\", \"That''s\":\"that is\", \"Do'nt\":\"do not\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0uJCB8V54hl"
   },
   "outputs": [],
   "source": [
    "def replace_symbol_special(text,check_vocab=False, vocab=None): \n",
    "\n",
    "    ''' \n",
    "    This method can be used to replace dashes ('-') around and within the words using regex.\n",
    "    It only removes dashes for words which are not known to the vocabluary.\n",
    "    Next to that, common word separators like underscores ('_') and slashes ('/') are replaced by spaces. \n",
    "    '''\n",
    "\n",
    "        \n",
    "    # replace all dashes and abostropes at the beginning of a word with a space\n",
    "    matches = re.findall(r\"\\s+(?:-|')\\w*\", text)\n",
    "    # if there is a match is in text\n",
    "    if len(matches) != 0:\n",
    "      # remove the dash from the match or better text\n",
    "      for match in matches:\n",
    "        text = re.sub(match, re.sub(r\"(?:-|')\", ' ', match), text)\n",
    "    \n",
    "    # replace all dashes and abostrophes at the end of a word with a space\n",
    "    # function works as above\n",
    "    matches = re.findall(r\"\\w*(?:-|')\\s+\", text)\n",
    "    if len(matches) != 0:\n",
    "      for match in matches:\n",
    "        text = re.sub(match, re.sub(r\"(?:-|')\", ' ', match), text)\n",
    "    \n",
    "    if check_vocab == True:\n",
    "      # replace dashes and abostrophes in the middle of the word only in case it is not known to a dictionary\n",
    "      # function works as above\n",
    "      matches = re.findall(r\"\\w*(?:-|')\\w*\", text)\n",
    "      if len(matches) != 0:\n",
    "        for match in matches:\n",
    "          #check if the word with dash in the middle in in the vocabluary\n",
    "          if match not in vocab.keys():\n",
    "            text = re.sub(match, re.sub(r\"(?:-|')\", ' ', match), text)\n",
    "    \n",
    "    #\n",
    "    text = re.sub(r'(?:_|\\/)', ' ', text)\n",
    "    \n",
    "    text = re.sub(r' +', ' ', text)#-\n",
    "    return text\n",
    "  \n",
    "# Initially we consideredto remove the dash for words with this beginning. \n",
    "# However we found that it had almost no impact. Applying it to the total text, would kill correct spellings.\n",
    "# pre_suffix_dict = {'bi-':'bi', \t'co-':'co','re-':'re',\t'de-':'de','pre-':'pre',\t'sub-':'sub', 'un-':'un'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bapKQaGs6DBH"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_smilies(text):\n",
    "  \n",
    "  '''\n",
    "  For investigation only: Find most common keyboard typed smilies in the text.\n",
    "  '''\n",
    "  \n",
    "  #define a pattern to find typical keyboard smilies\n",
    "  pattern = r\"((?:3|<)?(?::|;|=|B)(?:-|'|'-)?(?:\\)|D|P|\\*|\\(|o|O|\\]|\\[|\\||\\\\|\\/)\\s)\"\n",
    "  # Find the matches n the text\n",
    "  matches = re.findall(pattern, text)\n",
    "  # If the text contain matches print the text and the smilies found\n",
    "  if len(matches) != 0:\n",
    "    print(text, matches)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def replace_smilies(text):\n",
    "  \n",
    "  '''\n",
    "  Simplyfied method to replace keyboard smilies with its very simple translation.\n",
    "  '''\n",
    "  \n",
    "  #Find and replace all happy smilies\n",
    "  matches = re.findall(r\"((?:<|O|o|@)?(?::|;|=|B)(?:-|'|'-)?(?:\\)|\\]))\", text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r\"((?:<|O|o|@)?(?::|;|=|B)(?:-|'|'-)?(?:\\)|\\]))\", \" smile \", text)\n",
    "  \n",
    "  #Find and replace all laughing smilies\n",
    "  matches = re.findall(r\"((?:<)?(?::|;|=)(?:-|'|'-)?(?:d|D|P|p)\\b)\", text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r\"((?:<)?(?::|;|=)(?:-|'|'-)?(?:d|D|P|p)\\b)\", \" smile \", text)\n",
    "  \n",
    "  #Find and replace all unhappy smilies\n",
    "  matches = re.findall(r\"((?:3|<)?(?::|;|=|8)(?:-|'|'-)?(?:\\(|\\[|\\||\\\\|\\/))\", text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r\"((?:3|<)?(?::|;|=|8)(?:-|'|'-)?(?:\\(|\\[|\\||\\\\|\\/))\", \" unhappy \", text)\n",
    "  \n",
    "  #Find and replace all kissing smilies\n",
    "  matches = re.findall(r\"((?:<)?(?::|;|=)(?:-|'|'-)?(?:\\*))\", text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r\"((?:<)?(?::|;|=)(?:-|'|'-)?(?:\\*))\", \" kiss \", text)\n",
    "  \n",
    "  #Find and replace all surprised smilies\n",
    "  matches = re.findall(r\"((?::|;|=)(?:-|'|'-)?(?:o|O)\\b)\", text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r\"((?::|;|=)(?:-|'|'-)?(?:o|O)\\b)\", \" surprised \", text)\n",
    "    \n",
    "  #Find and replace all screaming smilies\n",
    "  matches = re.findall(r\"((?::|;|=)(?:-|'|'-)?(?:@)\\b)\", text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r\"((?::|;|=)(?:-|'|'-)?(?:@)\\b)\", \" screaming \", text)\n",
    "    \n",
    "  #Find and replace all hearts\n",
    "  matches = re.findall(r\"♥|❤|<3|❥|♡\", text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r\"(?:♥|❤|<3|❥|♡)\", \" love \", text)\n",
    "  \n",
    "  text = re.sub(' +', ' ',text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8v_vDFqHSblk"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stop_words):\n",
    "  \n",
    "  ''' \n",
    "  Remove stopwords and multiple whitespaces around words\n",
    "  '''\n",
    "  \n",
    "  #Compile stopwords separated by | and stopped by word boundary \n",
    "  stopword_re = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b')\n",
    "  # Replace the stopwords by space\n",
    "  text = stopword_re.sub(' ', text)\n",
    "  #Replace double spaces by a single space\n",
    "  text = re.sub(' +', ' ',text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8npB91fha_s"
   },
   "outputs": [],
   "source": [
    "def clean_text(text, scope='general'):\n",
    "  \n",
    "  '''\n",
    "  This function handles text cleaning from various symbols.\n",
    "  - it translates special font types into the standard text type of python.\n",
    "  - it removes all symbols except for dashes and abostrophes being handled by \n",
    "    \"replace_symbol_special\".\n",
    "  - it handles multi letter appearances like \"comiiii\" > \"comi\"\n",
    "  - typical unknown words like \"Trump\"\n",
    "  '''\n",
    "  \n",
    "  \n",
    "  \n",
    "  #compile all special symbols from the dictionary to one regex function\n",
    "  translate_regex = re.compile(r'(' + r'|'.join(translate_dictionary.keys()) + r')')\n",
    "  \n",
    "  # find all matches of special symbols in the text\n",
    "  matches = re.findall(translate_regex, text)\n",
    "  # if there is one or more matches\n",
    "  if len(matches) != 0:\n",
    "    for x in matches:\n",
    "      if x in translate_dictionary.keys():\n",
    "        #replace the symbol by its replacement item\n",
    "        text = re.sub(x, translate_dictionary.get(x), text)\n",
    "  \n",
    "  # find and remove all \"http\" links\n",
    "  matches = re.findall(r'http\\S+', text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "  \n",
    "  #remove all backslashes\n",
    "  matches = re.findall(r'\\\\', text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r'\\\\', ' ', text)\n",
    "  \n",
    "  # compile all remaining special characters into one translate line and replace them by space\n",
    "  # the translate function is really fast thus here our preferred choice\n",
    "  text = text.translate(str.maketrans(''.join(puncts), len(''.join(puncts))*' '))  \n",
    "  \n",
    "  #find words where 4 repetitions of a letter goes in a row and reduce them to only one\n",
    "  #we are not correcting words with 2 or three identical letters in a row as this could destroy correct words\n",
    "  #first find repeating characters\n",
    "  matches = re.findall(r'(.)\\1{3,}', text)\n",
    "  # is some are found\n",
    "  if len(matches) != 0:\n",
    "    #for each match replace it with its first letter (x[0])\n",
    "    for x in matches:\n",
    "      character_re = re.compile(x + '{3,}')\n",
    "      matchesInside = re.findall(character_re, text)\n",
    "      if len(matchesInside) != 0:\n",
    "        for x in matchesInside:\n",
    "          text = re.sub(x, x[0], text)\n",
    "          \n",
    "  # hahaha s by one haha \n",
    "  matches = re.findall(r'\\b[h,a]{4,}\\b', text)\n",
    "  if len(matches) != 0:\n",
    "    text = re.sub(r'\\b[h,a]{4,}\\b', 'haha', text)\n",
    "  \n",
    "  # as we found many unknown word variations including 'Trump' we reduce thse  words just to Trump\n",
    "  # being represented in most word vectors\n",
    "  matches = re.findall(r'\\w*[Tt][Rr][uU][mM][pP]\\w*', text)\n",
    "  if len(matches) != 0:\n",
    "    for x in matches:\n",
    "      text = re.sub(x, 'Trump', text)\n",
    "      \n",
    "  #remove potential double spaces generated during processing        \n",
    "  text = re.sub(' +', ' ',text) \n",
    "  \n",
    "  # those symbols are not touched by this function ->see replace_contraction or replace_special_symbols\n",
    "  #keep = [\"'\", '-', '´']\n",
    "  \n",
    "  \n",
    "  return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The dictionary was generated in the compare and investigation phase in the other notebook\n",
    "translate_dictionary = {'\\t': 't', '0': '0', '1': '1', '2': '2', '3': '3', '5': '5', '6': '6',\n",
    "                         '8': '8', '9': '9', 'd': 'd', 'e': 'e', 'h': 'h', 'm': 'm', 't': 't',\n",
    "                         '²': '2', '¹': '1', 'ĝ': 'g', 'œ': 'ae', 'ŝ': 's', 'ǧ': 'g', 'ɑ': 'ɑ',\n",
    "                         'ɒ': 'a', 'ɔ': 'c', 'ə': 'e', 'ɛ': 'e', 'ɡ': 'g', 'ɢ': 'g', 'ɪ': 'i',\n",
    "                         'ɴ': 'n', 'ʀ': 'r', 'ʏ': 'y', 'ʙ': 'b', 'ʜ': 'h', 'ʟ': 'l', 'ʰ': 'h',\n",
    "                         'ʳ': 'r', 'ʷ': 'w', 'ʸ': 'y', 'ˢ': '5', '͞': '-', '͟': '_', 'ͦ': 'o',\n",
    "                         'Α': 'a', 'Β': 'b', 'Ε': 'e', 'Μ': 'm', 'Ν': 'n', 'Ο': 'o', 'Τ': 't',\n",
    "                         'έ': 'e', 'ί': 'i', 'α': 'a', 'κ': 'k', 'χ': 'x', 'І': 'i', 'А': 'a',\n",
    "                         'Б': 'e', 'Е': 'e', 'З': '#', 'И': 'n', 'К': 'k', 'М': 'm', 'Н': 'h',\n",
    "                         'О': 'o', 'Р': 'p', 'С': 'c', 'У': 'y', 'Х': 'x',  'в': 'b',\n",
    "                         'к': 'k', 'м': 'm', 'н': 'h', 'ы': 'bi', 'ь': 'b', 'ё': 'e', 'љ': 'jb',\n",
    "                         'ғ': 'f', 'ү': 'y', 'Ԝ': 'w', 'հ': 'h', 'א': 'n', '௦': '0', '౦': 'o',\n",
    "                         '൦': 'o', '໐': 'o', 'Ꭵ': 'i', 'Ꭻ': 'j', 'Ꮷ': 'd', 'ᐨ': '-', 'ᐸ': '<',\n",
    "                         'ᑲ': 'b', 'ᑳ': 'b', 'ᗞ': 'd', 'ᴀ': 'a', 'ᴄ': 'c', 'ᴅ': 'n', 'ᴇ': 'e',\n",
    "                         'ᴊ': 'j', 'ᴋ': 'k', 'ᴍ': 'm', 'ᴏ': 'o', 'ᴑ': 'o', 'ᴘ': 'p', 'ᴛ': 't',\n",
    "                         'ᴜ': 'u', 'ᴠ': 'v', 'ᴡ': 'w', 'ᴵ': 'i', 'ᴷ': 'k', 'ᴺ': 'n', 'ᴼ': 'o',\n",
    "                         'ᵉ': 'e', 'ᵒ': 'o', 'ᵗ': 't', 'ᵘ': 'u', 'ẃ': 'w', 'ἀ': 'a', 'Ἀ': 'a',\n",
    "                         'Ἄ': 'a', 'ὶ': 'l', 'ὺ': 'u', '‒': '-', '₁': '1', '₃': '3', '₄': '4',\n",
    "                         'ℋ': 'h', '℠': 'sm', 'ℯ': 'e', 'ℴ': 'c', '╌': '--', 'ⲏ': 'h', 'ⲣ': 'p',\n",
    "                         '下': 'under', '不': 'Do not', '人': 'people', '伎': 'trick', '会': 'meeting',\n",
    "                         '作': 'Make', '你': 'you', '克': 'Gram', '关': 'turn off', '别': 'do not',\n",
    "                         '加': 'plus', '华': 'China', '卖': 'Sell', '去': 'go with', '哥': 'brother',\n",
    "                         '园': 'garden', '国': 'country', '圆': 'circle', '土': 'soil', '地': 'Ground',\n",
    "                         '坏': 'Bad', '外': 'outer', '大': 'Big', '失': 'Lost', '子': 'child', '小': 'small',\n",
    "                         '成': 'to make', '戦': 'War', '所': 'Place', '拿': 'take', '故': 'Therefore',\n",
    "                         '文': 'Text', '明': 'Bright', '是': 'Yes', '有': 'Have', '歌': 'song', \n",
    "                         '殊': 'special', '油': 'oil', '温': 'temperature', '特': 'special', \n",
    "                         '獄': 'prison', '的': 'of', '税': 'tax', '系': 'system', '群': 'group',\n",
    "                         '舞': 'dance', '英': 'English', '蔡': 'Cai', '议': 'Discussion', '谷': 'Valley',\n",
    "                         '豆': 'beans', '都': 'All', '钱': 'money', '降': 'drop', '障': 'barrier',\n",
    "                         '骗': 'cheat', '세': 'three', '안': 'within', '영': 'spirit', '요': 'Yo',\n",
    "                          'ͺ': '', 'Λ': 'L', 'Ξ': 'X', 'ά': 'a', 'ή': 'or', 'ι': 'j',\n",
    "                         'ξ': 'X', 'ς': 's', 'ψ': 't', 'ό': 'The', 'ύ': 'gt;', 'ώ': 'o',\n",
    "                         'ϖ': 'e.g.', 'Г': 'R', 'Д': 'D', 'Ж': 'F', 'Л': 'L', 'П': 'P', \n",
    "                         'Ф': 'F', 'Ш': 'Sh', 'б': 'b', 'п': 'P', 'ф': 'f', 'ц': 'c', \n",
    "                         'ч': 'no', 'ш': 'sh', 'щ': 'u', 'э': 'uh', 'ю': 'Yu', 'ї': 'her',\n",
    "                         'ћ': 'ht', 'Ձ': 'Winter', 'ա': 'a', 'դ': 'd', 'ե': 'e', 'ի': 's',\n",
    "                         'ձ': 'h', 'մ': 'm', 'յ': 'y', 'ն': 'h', 'ռ': 'r', 'ս': 'c', \n",
    "                         'ր': 'p', 'ւ': '³', 'ב': 'B', 'ד': 'D', 'ה': 'God', 'ו': 'and',\n",
    "                         'ט': 'ninth', 'י': 'J', 'ך': 'D', 'כ': 'about', 'ל': 'To', 'ם': 'From', \n",
    "                         'מ': 'M', 'ן': 'Estate', 'נ': 'N', 'ס': 'S.', 'ע': 'P', 'ף': 'Jeff',\n",
    "                         'פ': 'F', 'צ': 'C', 'ק': 'K.', 'ר': 'R.', 'ש': 'That', 'ת': 'A',\n",
    "                         'ء': 'Was', 'آ': 'Ah', 'أ': 'a', 'إ': 'a', 'ا': 'a', 'ة': 'e', \n",
    "                         'ت': 'T', 'ج': 'C', 'ح': 'H', 'خ': 'Huh', 'د': 'of the', 'ر': 'T',\n",
    "                         'ز': 'Z', 'س': 'Q', 'ش': 'Sh', 'ص': 's', 'ط': 'I', 'ع': 'AS', 'غ': 'G',\n",
    "                         'ف': 'F', 'ق': 'S', 'ك': 'K', 'ل': 'to', 'م': 'M', 'ن': 'N', 'ه': 'e', \n",
    "                         'و': 'And', 'ى': 'I', 'ي': 'Y', 'چ': 'What', 'ک': 'K', 'ی': 'Y', \n",
    "                         'क': 'A', 'म': 'M', 'र': 'And', 'ગ': 'C', 'જ': 'The same', \n",
    "                         'ત': 'I', 'ર': 'I', 'ஜ': 'SAD', 'ლ': 'L', 'ṑ': 'o', 'ἐ': 'e',\n",
    "                         'ἔ': 'Ë', 'ἡ': 'or', 'ἱ': 'ı', 'ἴ': 'i', 'ὀ': 'The', 'ὁ': 'The',\n",
    "                         'ὐ': 'ÿ', 'ὰ': 'a', 'ὲ': '.', 'ὸ': 'The', 'ύ': 'gt;', 'ᾶ': 'a', \n",
    "                         'ῆ': 'or', 'ῖ': 'ก', 'ῦ': 'I', 'う': 'U', 'さ': 'The', 'っ': 'What',\n",
    "                         'つ': 'One', 'な': 'The', 'よ': 'The', 'ら': 'Et al', 'エ': 'The', \n",
    "                         'ク': 'The', 'サ': 'The', 'シ': 'The', 'ジ': 'The', 'ス': 'The',\n",
    "                         'チ': 'The', 'ツ': 'The', 'ニ': 'D', 'ハ': 'Ha', 'マ': 'Ma', \n",
    "                         'リ': 'The', 'ル': 'Le', 'レ': 'Les', 'ロ': 'The', 'ン': 'The',\n",
    "                         '一': 'One', '与': 'versus', '且': 'And', '为': 'for', '买': 'buy',\n",
    "                         '了': 'Up', '些': 'some', '他': 'he', '以': 'Take', '们': 'They',\n",
    "                         '件': 'Items', '传': 'pass', '伦': 'Lun', '但': 'but', '信': 'letter',\n",
    "                         '候': 'Waiting', '偽': 'Pseudo', '全': 'all', '公': 'public', '其': 'its',\n",
    "                         '养': 'support', '冬': 'winter', '凸': 'Convex', '击': 'hit', '判': 'Judge',\n",
    "                         '到': 'To', '友': 'Friend', '可': 'can', '吗': 'What?', '和': 'with',\n",
    "                         '唯': 'only', '因': 'because', '圣': 'Holy', '在': 'in', '基': 'base',\n",
    "                         '堂': 'Hall', '士': 'Shishi', '复': 'complex', '多': 'many', '天': 'day',\n",
    "                         '好': 'it is good', '如': 'Such as', '婚': 'marriage', '孩': 'child', \n",
    "                         '宠': 'Pet', '寓': 'Apartment', '对': 'Correct', '屁': 'fart', \n",
    "                         '屈': 'Qu', '巨': 'huge', '己': 'already', '式': 'formula', '当': 'when',\n",
    "                         '彼': 'he', '徒': 'only', '得': 'Got', '怒': 'angry', '怪': 'strange',\n",
    "                         '恐': 'fear', '惧': 'fear', '想': 'miss you', '愤': 'anger', '我': 'I',\n",
    "                         '战': 'war', '批': 'Batch', '把': 'Put', '拉': 'Pull', '拷': 'Copy', \n",
    "                         '接': 'Connect', '操': 'Fuck', '收': 'Receive', '政': 'Politics', \n",
    "                         '教': 'teach', '斤': 'jin', '斯': 'S', '新': 'new', '时': 'Time', \n",
    "                         '普': 'general', '曾': 'Once', '本': 'this', '杀': 'kill', '极': 'pole',\n",
    "                         '查': 'check', '栗': 'chestnut', '株': 'stock', '样': 'kind', '检': 'Check',\n",
    "                         '欢': 'Happy', '死': 'dead', '汉': 'Chinese', '没': 'No', '治': 'rule', \n",
    "                         '法': 'law', '活': 'live', '点': 'point', '燻': 'Moth', '物': 'object',\n",
    "                         '猜': 'guess', '猴': 'monkey', '理': 'Rational', '生': 'Health', '用': 'use',\n",
    "                         '白': 'White', '百': 'hundred', '直': 'straight', '相': 'phase', '看': 'Look',\n",
    "                         '督': 'Supervisor', '知': 'know', '社': 'Society', '祝': 'wish', '积': 'product',\n",
    "                         '稣': 'Jesus', '经': 'through', '结': 'Knot', '给': 'give', '美': 'nice', \n",
    "                         '耶': 'Yay', '聊': 'chat', '胜': 'Win', '至': 'to', '虚': 'Virtual', '製': 'Made', \n",
    "                         '要': 'Want', '认': 'recognize', '讨': 'discuss', '让': 'Let', '识': 'knowledge',\n",
    "                         '话': 'words', '语': 'language', '说': 'Say', '谊': 'friendship', \n",
    "                         '谓': 'Predicate', '象': 'Elephant', '贺': 'He', '赢': 'win', '迎': 'welcome',\n",
    "                         '还': 'also', '这': 'This', '通': 'through', '鉄': 'iron', '问': 'ask', \n",
    "                         '阿': 'A', '题': 'question', '额': 'amount', '鬼': 'ghost', '鸡': 'Chicken',\n",
    "                         '가': 'end', '갈': 'Go', '게': 'to', '격': 'case', '경': 'circa', '관': 'tube',\n",
    "                         '국': 'soup', '금': 'gold', '나': 'I', '는': 'The', '니': 'Nee', '다': 'All',\n",
    "                         '대': 'versus', '도': 'Degree', '된': 'The', '드': 'De', '들': 'field', \n",
    "                         '때': 'time', '런': 'Run', '렵': 'Hi', '록': 'rock', '뤼': 'Crown', \n",
    "                         '리': 'Lee', '마': 'hemp', '만': 'just', '반': 'half', '분': 'minute', \n",
    "                         '사': 'four', '상': 'Prize', '서': 'book', '석': 'three', '성': 'castle',\n",
    "                         '스': 'The', '시': 'city', '않': 'Not', '야': 'Hey', '약': 'about', \n",
    "                         '어': 'uh', '와': 'Wow', '용': 'for', '유': 'U', '을': 'of', '이': 'this',\n",
    "                         '인': 'sign', '잘': 'well', '제': 'My', '쥐': 'rat', '지': 'G', '초': 'second',\n",
    "                         '캐': 'Can', '탱': 'Tang', '트': 'The', '티': 'tea', '패': 'tile', '품': 'Width', \n",
    "                         '한': 'One', '합': 'synthesis', '해': 'year', '허': 'Huh', '화': 'anger', '황': 'sulfur',\n",
    "                         '하': 'Ha', 'ﬁ': 'be', '０': '#', '２': '#', '８': '#', 'Ｅ': 'e', 'Ｇ': 'g',\n",
    "                         'Ｈ': 'h', 'Ｍ': 'm', 'Ｎ': 'n', 'Ｏ': 'O', 'Ｓ': 's', 'Ｕ': 'U', 'Ｗ': 'w',\n",
    "                         'ａ': 'a', 'ｂ': 'b', 'ｃ': 'c', 'ｄ': 'd', 'ｅ': 'e', 'ｆ': 'f', 'ｇ': 'g',\n",
    "                         'ｈ': 'h', 'ｉ': 'i', 'ｋ': 'k', 'ｌ': 'l', 'ｍ': 'm', 'ｎ': 'n', 'ｏ': 'o',\n",
    "                         'ｒ': 'r', 'ｓ': 's', 'ｔ': 't', 'ｕ': 'u', 'ｖ': 'v', 'ｗ': 'w', 'ｙ': 'y',\n",
    "                         '𝐀': 'a', '𝐂': 'c', '𝐃': 'd', '𝐅': 'f', '𝐇': 'h', '𝐊': 'k', '𝐍': 'n', \n",
    "                         '𝐎': 'o', '𝐑': 'r', '𝐓': 't', '𝐔': 'u', '𝐘': 'y', '𝐙': 'z', '𝐚': 'a',\n",
    "                         '𝐛': 'b', '𝐜': 'c', '𝐝': 'd', '𝐞': 'e', '𝐟': 'f', '𝐠': 'g', '𝐡': 'h', \n",
    "                         '𝐢': 'i', '𝐣': 'j', '𝐥': 'i', '𝐦': 'm', '𝐧': 'n', '𝐨': 'o', '𝐩': 'p',\n",
    "                         '𝐪': 'q', '𝐫': 'r', '𝐬': 's', '𝐭': 't', '𝐮': 'u', '𝐯': 'v', '𝐰': 'w',\n",
    "                         '𝐱': 'x', '𝐲': 'y', '𝐳': 'z', '𝑥': 'x', '𝑦': 'y', '𝑧': 'z', '𝑩': 'b',\n",
    "                         '𝑪': 'c', '𝑫': 'd', '𝑬': 'e', '𝑭': 'f', '𝑮': 'g', '𝑯': 'h', '𝑰': 'i',\n",
    "                         '𝑱': 'j', '𝑲': 'k', '𝑳': 'l', '𝑴': 'm', '𝑵': 'n', '𝑶': '0', '𝑷': 'p',\n",
    "                         '𝑹': 'r', '𝑺': 's', '𝑻': 't', '𝑾': 'w', '𝒀': 'y', '𝒁': 'z', '𝒂': 'a',\n",
    "                         '𝒃': 'b', '𝒄': 'c', '𝒅': 'd', '𝒆': 'e', '𝒇': 'f', '𝒈': 'g', '𝒉': 'h',\n",
    "                         '𝒊': 'i', '𝒋': 'j', '𝒌': 'k', '𝒍': 'l', '𝒎': 'm', '𝒏': 'n', '𝒐': 'o', \n",
    "                         '𝒑': 'p', '𝒒': 'q', '𝒓': 'r', '𝒔': 's', '𝒕': 't', '𝒖': 'u', '𝒗': 'v', \n",
    "                         '𝒘': 'w', '𝒙': 'x', '𝒚': 'y', '𝒛': 'z', '𝒩': 'n', '𝒶': 'a', '𝒸': 'c',\n",
    "                         '𝒽': 'h', '𝒾': 'i', '𝓀': 'k', '𝓁': 'l', '𝓃': 'n', '𝓅': 'p', '𝓇': 'r',\n",
    "                         '𝓈': 's', '𝓉': 't', '𝓊': 'u', '𝓌': 'w', '𝓎': 'y', '𝓒': 'c', '𝓬': 'c',\n",
    "                         '𝓮': 'e', '𝓲': 'i', '𝓴': 'k', '𝓵': 'l', '𝓻': 'r', '𝓼': 's', '𝓽': 't',\n",
    "                         '𝓿': 'v', '𝕴': 'j', '𝕸': 'm', '𝕿': 'i', '𝖂': 'm', '𝖆': 'a', '𝖇': 'b',\n",
    "                         '𝖈': 'c', '𝖉': 'd', '𝖊': 'e', '𝖋': 'f', '𝖌': 'g', '𝖍': 'h', '𝖎': 'i', \n",
    "                         '𝖒': 'm', '𝖓': 'n', '𝖕': 'p', '𝖗': 'r', '𝖘': 's', '𝖙': 't', '𝖚': 'u',\n",
    "                         '𝖛': 'v', '𝖜': 'w', '𝖞': 'n', '𝖟': 'z', '𝗕': 'b', '𝗘': 'e', '𝗙': 'f',\n",
    "                         '𝗞': 'k', '𝗟': 'l', '𝗠': 'm', '𝗢': 'o', '𝗤': 'q', '𝗦': 's', '𝗧': 't',\n",
    "                         '𝗪': 'w', '𝗭': 'z', '𝗮': 'a', '𝗯': 'b', '𝗰': 'c', '𝗱': 'd', '𝗲': 'e',\n",
    "                         '𝗳': 'f', '𝗴': 'g', '𝗵': 'h', '𝗶': 'i', '𝗷': 'j', '𝗸': 'k', '𝗹': 'i',\n",
    "                         '𝗺': 'm', '𝗻': 'n', '𝗼': 'o', '𝗽': 'p', '𝗿': 'r', '𝘀': 's', '𝘁': 't',\n",
    "                         '𝘂': 'u', '𝘃': 'v', '𝘄': 'w', '𝘅': 'x', '𝘆': 'y', '𝘇': 'z', '𝘐': 'l',\n",
    "                         '𝘓': 'l', '𝘖': 'o', '𝘢': 'a', '𝘣': 'b', '𝘤': 'c', '𝘥': 'd', '𝘦': 'e',\n",
    "                         '𝘧': 'f', '𝘨': 'g', '𝘩': 'h', '𝘪': 'i', '𝘫': 'j', '𝘬': 'k', '𝘮': 'm',\n",
    "                         '𝘯': 'n', '𝘰': 'o', '𝘱': 'p', '𝘲': 'q', '𝘳': 'r', '𝘴': 's', '𝘵': 't',\n",
    "                         '𝘶': 'u', '𝘷': 'v', '𝘸': 'w', '𝘹': 'x', '𝘺': 'y', '𝘼': 'a', '𝘽': 'b',\n",
    "                         '𝘾': 'c', '𝘿': 'd', '𝙀': 'e', '𝙃': 'h', '𝙅': 'j', '𝙆': 'k', '𝙇': 'l', \n",
    "                         '𝙈': 'm', '𝙊': 'o', '𝙋': 'p', '𝙍': 'r', '𝙏': 't', '𝙒': 'w', '𝙔': 'y',\n",
    "                         '𝙖': 'a', '𝙗': 'b', '𝙘': 'c', '𝙙': 'd', '𝙚': 'e', '𝙛': 'f', '𝙜': 'g',\n",
    "                         '𝙝': 'h', '𝙞': 'i', '𝙟': 'j', '𝙠': 'k', '𝙢': 'm', '𝙣': 'n', '𝙤': 'o',\n",
    "                         '𝙥': 'p', '𝙧': 'r', '𝙨': 's', '𝙩': 't', '𝙪': 'u', '𝙫': 'v', '𝙬': 'w',\n",
    "                         '𝙭': 'x', '𝙮': 'y', '𝟎': '0', '𝟏': '1', '𝟐': '2', '𝟓': '5', '𝟔': '6',\n",
    "                         '𝟖': '8', '𝟬': '0', '𝟭': '1', '𝟮': '2', '𝟯': '3', '𝟰': '4', '𝟱': '5',\n",
    "                         '𝟲': '6', '𝟳': '7', '𝟑':'3', '𝟒':'4', '𝟕':'7', '𝟗':'9',\n",
    "                         '🇦': 'a', '🇩': 'd', '🇪': 'e', '🇬': 'g', '🇮': 'i', \n",
    "                         '🇳': 'n', '🇴': 'o', '🇷': 'r', '🇹': 't', '🇼': 'w', '🖒': 'thumps up',\n",
    "                         'ℏ':'h', 'ʲ':'j', 'Ｃ':'c', 'ĺ':'i', 'Ｊ':'j', 'ĸ':'k', 'Ｐ':'p'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List was cerated in separate notebook investigating on word embedding. \n",
    "# These dictionary is used to remove unwanted characters from the text\n",
    "puncts =                 ['_','!', '?','\\x08', '\\n', '\\x0b', '\\r', '\\x10', '\\x13', '\\x1f', ' ', ' # ', '\"', '#', \n",
    "                         '# ', '$', '%', '&',  '(', ')', '*', '+', ',',  '/', '.', ':', ';', '<',\n",
    "                         '=', '>', '@', '[', '\\\\', ']', '^', '`', '{', '|', '}', '~', '\\x7f', '\\x80',\n",
    "                         '\\x81', '\\x85', '\\x91', '\\x92', '\\x95', '\\x96', '\\x9c', '\\x9d', '\\x9f', '\\xa0', \n",
    "                         '¡', '¢༼', '£', '¤', '¥', '§', '¨', '©', '«', '¬', '\\xad', '¯', '°', '±', '³',\n",
    "                         '¶', '·', '¸', 'º', '»', '¼', '½', '¾', '¿', '×', 'Ø', '÷', 'ø', 'Ƅ', 'ƽ',\n",
    "                         'ǔ', 'Ȼ', 'ɜ', 'ɩ', 'ʃ', 'ʌ', 'ʻ', 'ʼ', 'ˈ', 'ˌ', 'ː', '˙', '˚', '́', '̄', '̅', \n",
    "                         '̇', '̈', '̣', '̨', '̯', '̱', '̲', '̶', '͜', '͝', '͞', '͟', '͡', 'ͦ', '؟', 'َ', 'ِ', 'ڡ', \n",
    "                         '۞', '۩', '܁', 'ा', '्', 'ા', 'ી', 'ુ', '๏', '๏̯͡', '༼', '༽', 'ᐃ', 'ᐣ', 'ᐦ', 'ᐧ',\n",
    "                         'ᑎ', 'ᑭ', 'ᑯ', 'ᒧ', 'ᓀ', 'ᓂ', 'ᓃ', 'ᓇ', 'ᔭ', 'ᴦ', 'ᴨ', 'ᵻ', 'Ἰ', 'Ἱ', 'ὼ', \n",
    "                         '᾽', 'ῃ', '\\u2000', '\\u2001', '\\u2002', '\\u2003', '\\u2004', '\\u2005', '\\u2006', \n",
    "                         '\\u2007', '\\u2008', '\\u2009', '\\u200a', '\\u200b', '\\u200c', '\\u200d', '\\u200e',\n",
    "                         '\\u200f', '‐', '‑', '‒', '–', '—', '―', '‖', '‘', '’', '‚', '‛', '“', '”', '„',\n",
    "                         '†', '‡', '•', '‣', '…', '\\u2028', '\\u202a', '\\u202c', '\\u202d', '\\u202f', '‰',\n",
    "                         '′', '″', '‹', '›', '‿', '⁄', '⁍̴̛\\u3000', '⁎', '⁴', '₂', '€', '₵', '₽', '℃', '℅',\n",
    "                         'ℐ', '™', '℮', '⅓', '←', '↑', '→', '↓', '↳', '↴', '↺', '⇌', '⇒', '⇤', '∆', '∎',\n",
    "                         '∏', '−', '∕', '∙', '√', '∞', '∩', '∴', '∵', '∼', '≈', '≠', '≤', '≥', '⊂', '⊕',\n",
    "                         '⊘', '⋅', '⋆', '⌠', '⎌', '⏖', '─', '━', '┃', '┈', '┊', '┗', '┣', '┫', '┳', '╌', '═',\n",
    "                         '║', '╔', '╗', '╚', '╣', '╦', '╩', '╪', '╭', '╭╮', '╮', '╯', '╰', '╱', '╲', '▀',\n",
    "                         '▂', '▃', '▄', '▅', '▆', '▇', '█', '▊', '▋', '▏', '░', '▒', '▓', '▔', '▕', \n",
    "                         '▙', '■', '▪', '▬', '▰', '▱', '▲', '▷', '▸', '►', '▼', '▾', '◄', '◇', '○',\n",
    "                         '●', '◐', '◔', '◕', '◝', '◞', '◡', '◦', '★', '☆', '☏', '☐', '☒', '☙', '☛',\n",
    "                         '☜', '☞', '☭', '☻', '☼', '♦', '♩', '♪', '♫', '♬', '♭', '♲', '⚆', '⚭', '⚲', '✀',\n",
    "                         '✓', '✘', '✞', '✧', '✬', '✭', '✰', '✾', '❆', '❧', '➤', '➥', '⠀', '⤏', '⦁',\n",
    "                         '⩛', '⬭', '⬯', '\\u3000', '、', '。', '《', '》', '「', '」', '〔', '・', 'ㄸ', 'ㅓ',\n",
    "                         '锟', 'ꜥ', '\\ue014', '\\ue600', '\\ue602', '\\ue607', '\\ue608', '\\ue613', '\\ue807',\n",
    "                         '\\uf005', '\\uf020', '\\uf04a', '\\uf04c', '\\uf070',  '\\uf202\\uf099', '\\uf203',\n",
    "                         '\\uf071\\uf03d\\uf031\\uf02f\\uf032\\uf028\\uf070\\uf02f\\uf032\\uf02d\\uf061\\uf029',\n",
    "                         '\\uf099', '\\uf09a', '\\uf0a7', '\\uf0b7', '\\uf0e0', '\\uf10a', '\\uf202', \n",
    "                         '\\uf203\\uf09a', '\\uf222', '\\uf222\\ue608', '\\uf410', '\\uf410\\ue600', '\\uf469', \n",
    "                         '\\uf469\\ue607', '\\uf818', '﴾', '﴾͡', '﴿', 'ﷻ', '\\ufeff', '！', '％', '＇',\n",
    "                         '（', '）', '，', '－', '．', '／', '：', '＞', '？', '＼', '｜', '￦', '￼', '�',\n",
    "                         '𝒻', '𝕾', '𝖄', '𝖐', '𝖑', '𝖔', '𝗜', '𝘊', '𝘭', '𝙄', '𝙡', '𝝈', '🖑', '🖒']\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yiVIAY6sh5VV"
   },
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "  \n",
    "  \"\"\"\n",
    "  The following function is used to format the numbers.\n",
    "  In the beginning \"th, st, nd, rd\" are removed\n",
    "  \"\"\"\n",
    "  \n",
    "  #remove \"th\" after a number\n",
    "  matches = re.findall(r'\\b\\d+\\s*th\\b', x)\n",
    "  if len(matches) != 0:\n",
    "    x = re.sub(r'\\s*th\\b', \" \", x)\n",
    "    \n",
    "  #remove \"rd\" after a number \n",
    "  matches = re.findall(r'\\b\\d+\\s*rd\\b', x)\n",
    "  if len(matches) != 0:\n",
    "    x = re.sub(r'\\s*rd\\b', \" \", x)\n",
    "  \n",
    "  #remove \"st\" after a number\n",
    "  matches = re.findall(r'\\b\\d+\\s*st\\b', x)\n",
    "  if len(matches) != 0:\n",
    "    x = re.sub(r'\\s*st\\b', \" \", x)\n",
    "    \n",
    "  #remove \"nd\" after a number\n",
    "  matches = re.findall(r'\\b\\d+\\s*nd\\b', x)\n",
    "  if len(matches) != 0:\n",
    "    x = re.sub(r'\\s*nd\\b', \" \", x)\n",
    "  \n",
    "  # replace standalone numbers higher than 10 by #\n",
    "  # this function does not touch numbers linked to words like \"G-20\"\n",
    "  matches = re.findall(r'^\\d+\\s+|\\s+\\d+\\s+|\\s+\\d+$', x)\n",
    "  if len(matches) != 0:\n",
    "    x = re.sub('^[0-9]{5,}\\s+|\\s+[0-9]{5,}\\s+|\\s+[0-9]{5,}$', ' ##### ', x)\n",
    "    x = re.sub('^[0-9]{4}\\s+|\\s+[0-9]{4}\\s+|\\s+[0-9]{4}$', ' #### ', x)\n",
    "    x = re.sub('^[0-9]{3}\\s+|\\s+[0-9]{3}\\s+|\\s+[0-9]{3}$', ' ### ', x)\n",
    "    x = re.sub('^[0-9]{2}\\s+|\\s+[0-9]{2}\\s+|\\s+[0-9]{2}$', ' ## ', x)\n",
    "    #we do include the range from 1 to 10 as all word-vectors include them\n",
    "    #x = re.sub('[0-9]{1}', '#', x)\n",
    "    \n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ko0nNEfqzUA7"
   },
   "outputs": [],
   "source": [
    "def year_and_hour(text):\n",
    "  \"\"\"\n",
    "  This function is used to replace \"yr,yrs\" by year and \"hr,hrs\" by hour.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Find matches for \"yr\", \"yrs\", \"hr\", \"hrs\"\n",
    "  matches_year = re.findall(r'\\b\\d+\\s*yr\\b', text)\n",
    "  matches_years = re.findall(r'\\b\\d+\\s*yrs\\b', text)\n",
    "  matches_hour = re.findall(r'\\b\\d+\\s*hr\\b', text)\n",
    "  matches_hours = re.findall(r'\\b\\d+\\s*hrs\\b', text)\n",
    "  \n",
    "  # replace all matches accordingly\n",
    "  if len(matches_year) != 0:\n",
    "    text = re.sub(r'\\b\\d+\\s*yr\\b', \"year\", text)\n",
    "  if len(matches_years) != 0:\n",
    "    text = re.sub(r'\\b\\d+\\s*yrs\\b', \"year\", text)\n",
    "  if len(matches_hour) != 0:\n",
    "    text = re.sub(r'\\b\\d+\\s*hr\\b', \"hour\", text)\n",
    "  if len(matches_hours) != 0:\n",
    "    text = re.sub(r'\\b\\d+\\s*hrs\\b', \"hour\", text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpCT9Z851nWK"
   },
   "outputs": [],
   "source": [
    "def textBlobLemmatize(sentence):\n",
    "  \"\"\"\n",
    "  This function uses the Word lemmatizer function of the textBlob package.\n",
    "  \"\"\"  \n",
    "  #for each word in the text, replace the word by its lemmatized version\n",
    "  for x in sentence.split():\n",
    "    sentence = sentence.replace(x, Word(x).lemmatize())\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HG00eRA242KW"
   },
   "outputs": [],
   "source": [
    "def build_vocab(df):\n",
    "  \n",
    "  '''Build a dictionary of words and its number of occurences from the data frame'''\n",
    "  \n",
    "  #initialize the tokenizer\n",
    "  tokenizer = TweetTokenizer()\n",
    "  \n",
    "  vocab = {}\n",
    "  for i, row in enumerate(df):\n",
    "      #tokenize the sentence \n",
    "      words = tokenizer.tokenize(row)\n",
    "      #for each word, check if it is in the dict otherwise add a new entry\n",
    "      for w in words:\n",
    "       \n",
    "        try:\n",
    "            vocab[w] += 1\n",
    "        except KeyError:\n",
    "            vocab[w] = 1\n",
    "  \n",
    "  return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rIDA-Ib4dV4"
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda\n",
    "def check_coverage(vocab,embeddings_index, print_oov_num=100):\n",
    "  '''\n",
    "  This function checks what part of the vocabluary and the text is covered by the embedding index.\n",
    "  It returns a list of tuples of unknown words and its occuring frequency.\n",
    "  '''\n",
    "  \n",
    "  a = {}\n",
    "  oov = {}\n",
    "  k = 0\n",
    "  i = 0\n",
    "\n",
    "  # for every word in vocab\n",
    "  for word in vocab:\n",
    "      # check if it can be found in the embedding\n",
    "      try:\n",
    "          # store the embedding index to a\n",
    "          a[word] = embeddings_index[word]\n",
    "          # count up by #of occurences in df\n",
    "          k += vocab[word]\n",
    "      except:\n",
    "          # if no embedding for word, add to oov\n",
    "          oov[word] = vocab[word]\n",
    "          # # count up by #of occurences in df\n",
    "          i += vocab[word]\n",
    "          pass\n",
    "  # calc percentage of #of found words by length of vocab\n",
    "  print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "  # devide number of found words by number of all words from df\n",
    "  print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "\n",
    "  # return unknown words sorted by number of occurences\n",
    "  sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "  print('Top unknown words are:', sorted_x[:print_oov_num])\n",
    "\n",
    "  #return dict of unknown words + occurences\n",
    "  return oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4BtKz-ANCfJ"
   },
   "outputs": [],
   "source": [
    "def  load_embedding_vocab(path):\n",
    "  '''\n",
    "  Load the embeddings in the right format and return the vocab dictionary. \n",
    "  '''  \n",
    "  # Print starting info about the pre-processing\n",
    "  starttime = datetime.datetime.now().replace(microsecond=0)\n",
    "  print(\"Starttime: \", starttime)\n",
    "\n",
    "  def timediff(time):\n",
    "    return time - starttime\n",
    "  \n",
    "  EMBEDDING_FILE = path\n",
    "  def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "  embeddings_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(EMBEDDING_FILE)) \n",
    "    \n",
    "  time = datetime.datetime.now().replace(microsecond=0)\n",
    "  print(\"Embedding model loaded and vocab returned. Time since start: \", timediff(time))\n",
    "  \n",
    "  #return the vocab\n",
    "  return embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dz3JY_SiItlR"
   },
   "source": [
    "### Pre-processing conventional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-M_a9QkhFVm"
   },
   "source": [
    "### Pre-processing for neural networks with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uU60o9V2pCXF"
   },
   "source": [
    "The aim is to **maximize the true embedding coverage** of the dataset **only tuning when needed.**\n",
    "\n",
    "As main input the function takes the dataset, the embedding vocabulary.\n",
    "1. Vocabulary from data set is build\n",
    "2. The coverage of the dataset vocabulary is compared to the embedding vocabulary\n",
    "3. Unknown words (oov = out of vocabulary) are identified\n",
    "4. The relevant pre-processings are applied towards the unknown words only \n",
    "5. A dictionary with unknown word and its correction is generated, words which could not get improved are ignored\n",
    "6. Unknown words in the text are replaced \n",
    "7. Dataset returned in a way that every word is separated by space.\n",
    "\n",
    "Thus anyone who wants to use the **outcome of this pre-processing only needs to separate the words by space ( ' ' ).**\n",
    "\n",
    "To tokenize the words in a good way we used the **TweetTokenizer,** which we found most powerful for our challenge. It keeps symbols for smilies together, however separates punctuation at the end of a sentence. Next to that it appeared to be extremely fast! We could reduce our proprocesing speed by more than 20 minutes on the full 1.8 million dataset, so that the method including coverage check takes about 16 minutes. This will be explained more in detail in the next part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0p7E_d7gQ2KF"
   },
   "source": [
    "#### Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsvFFGvwYreC"
   },
   "outputs": [],
   "source": [
    "def preprocessing_NN(df, model_vocab, calc_coverage=True, print_oov_num=100):\n",
    "  \"\"\"\n",
    "  This function is only correcting words which are not out of the box known towards the embedding dictionary.\n",
    "  It is optimized using the nltk TweetTokenizer.\n",
    "\n",
    "  Function that combines the whole pre-processing process specifically for neural networks where less pre-processing is required compared to conventional methods.\n",
    "  This means we will not remove stopwords, lemmatize or remove typical punctuation.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Set parameters\n",
    "  tokenizer = TweetTokenizer()\n",
    "  \n",
    "  # Print starting info about the pre-processing\n",
    "  starttime = datetime.datetime.now().replace(microsecond=0)\n",
    "  print('Dataset Length: ', len(df), \"Starttime: \", starttime)\n",
    "\n",
    "  def timediff(time):\n",
    "    return time - starttime\n",
    "  \n",
    "  # build a vocabulary from the text \n",
    "  vocab = build_vocab(df.comment_text)\n",
    "  print('Embedding vectors are loaded. \\n')\n",
    "  # check the coverage and receive a dictionary of unknown words\n",
    "  unknown = check_coverage(vocab,model_vocab, print_oov_num=print_oov_num)\n",
    "  # extract the list of unknown words\n",
    "  unknown = unknown.keys()\n",
    "  \n",
    "  ## Process the unknown words\n",
    "  # The replace_contractions function is applied on the data frame\n",
    "  corrected = [replace_contractions(x) for x in unknown]\n",
    "  time = datetime.datetime.now().replace(microsecond=0)\n",
    "  print(\"Contractions have been replaced. Time since start: \", timediff(time))\n",
    "\n",
    "  # Replace emojis with text\n",
    "  corrected = [emoji.demojize(x) for x in corrected]\n",
    "  time = datetime.datetime.now().replace(microsecond=0)\n",
    "  print(\"Emojis have been converted to text. Time since start: \", timediff(time))\n",
    "\n",
    "  # Replace keyboard smilies with text\n",
    "  corrected = [replace_smilies(x) for x in corrected]\n",
    "  time = datetime.datetime.now().replace(microsecond=0)\n",
    "  print(\"Smilies have been converted to text. Time since start: \", timediff(time))\n",
    "\n",
    "  # The clean_text function is applied on the data frame\n",
    "  corrected = [clean_text(x) for x in corrected]\n",
    "  time = datetime.datetime.now().replace(microsecond=0)\n",
    "  print(\"All signs have been removed. Time since start: \", timediff(time))\n",
    "  \n",
    "  # The clean_numbers function is applied\n",
    "  corrected = [clean_numbers(x) for x in corrected]\n",
    "  time = datetime.datetime.now().replace(microsecond=0)\n",
    "  print(\"All numbers have been replaced with ###. Time since start: \", timediff(time))\n",
    "  \n",
    "    # Replace or remove special characters like - / _ according to rules\n",
    "  corrected = [replace_symbol_special(x, check_vocab=True, vocab=model_vocab) for x in corrected]\n",
    "  time = datetime.datetime.now().replace(microsecond=0)\n",
    "  print(\"Special symbols have been processed. Time since start: \", timediff(time))\n",
    "\n",
    "  # Abbreviations are replaced by year and hour\n",
    "  corrected = [year_and_hour(x) for x in corrected]\n",
    "  time = datetime.datetime.now().replace(microsecond=0)\n",
    "  print(\"Yr and hr have been replaced by year and hour. Time since start: \", timediff(time))\n",
    "  \n",
    "  # *Takes too long\n",
    "  #Correct spelling mistakes\n",
    "  #corrected = [TextBlob(x).correct() for x in corrected]\n",
    "  #time = datetime.datetime.now().replace(microsecond=0)\n",
    "  #print(\"Yr and hr have been replaced by year and hour. Time since start: \", timediff(time))\n",
    "  \n",
    "  #create a dictionary from word and correction\n",
    "  dictionary = dict(zip(unknown, corrected))\n",
    "  keys = dictionary.keys()\n",
    "  \n",
    "  #remove all keys where unknown equals correction after processing\n",
    "  #create a new dict\n",
    "  dict_mispell = dict()\n",
    "  for key in dictionary.keys():\n",
    "    # if the correction differs from the unknown word add it to the new dict\n",
    "    if key != dictionary.get(key):\n",
    "      dict_mispell[key] = dictionary.get(key)\n",
    "  \n",
    "  time = datetime.datetime.now().replace(microsecond=0)\n",
    "  print('Correction dictionary of unknown words prepared. Time since start: ', timediff(time))\n",
    "  #print(dict_mispell, '\\n')\n",
    "  \n",
    "  def clean_mispell(text, dict_mispell):\n",
    "    '''Replaces the unknown words in the text by its corrections.'''\n",
    "    #tokenize the text with TweetTokenizer\n",
    "    words = tokenizer.tokenize(text)\n",
    "    for i, word in enumerate(words):\n",
    "      # if the word is among the misspellings\n",
    "      if word in dict_mispell.keys():\n",
    "        #replace it by the corrected word\n",
    "        words[i] = dict_mispell.get(word)\n",
    "    #merge text by space\n",
    "    text = ' '.join(words)\n",
    "    # remove all double spaces potentially appearing after pre-processing.\n",
    "    text  = re.sub(r' +', ' ', text)\n",
    "    return text\n",
    "      \n",
    "  \n",
    "  #tqdm.pandas()\n",
    "  df.comment_text = df.comment_text.apply(lambda x: clean_mispell(x, dict_mispell))\n",
    "  time = datetime.datetime.now().replace(microsecond=0)\n",
    "  print('Unknown words replaced excluding coverage check. Time since start: ', timediff(time))\n",
    "  \n",
    "  # print the final result\n",
    "  if calc_coverage == True: \n",
    "    vocab = build_vocab(df.comment_text)\n",
    "    unknown = check_coverage(vocab,model_vocab, print_oov_num=print_oov_num)\n",
    "    time = datetime.datetime.now().replace(microsecond=0)\n",
    "    print('Pre-processing done including coverage check. Time since start: ', timediff(time))\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-s9tdBwPZLY"
   },
   "source": [
    "#### Test pre-processing neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starttime:  2019-06-19 22:24:57\n",
      "Embedding model loaded and vocab returned. Time since start:  0:02:52\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_vocab_glove = load_embedding_vocab('../input/glove840b300dtxt/glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZT6RgHycxpNg"
   },
   "source": [
    "**Train Set Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv\")\n",
    "# kill all other columns except comment text\n",
    "cols_to_keep = ['comment_text','target']\n",
    "train_data = train_data.drop(train_data.columns.difference(cols_to_keep), axis=1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "colab_type": "code",
    "id": "e75gdI49O8ZN",
    "outputId": "3d8ce936-a7cc-4811-b11e-43600cfe5c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Glove \n",
      "\n",
      "Dataset Length:  1804874 Starttime:  2019-06-19 22:28:06\n",
      "Embedding vectors are loaded. \n",
      "\n",
      "Found embeddings for 49.62% of vocab\n",
      "Found embeddings for  98.69% of all text\n",
      "Top unknown words are: [('..', 43329), (\"isn't\", 41938), (\"That's\", 38281), (\"won't\", 30977), (\"he's\", 24986), ('\\xad', 24600), (\"Trump's\", 24097), (\"aren't\", 21485), (\"wouldn't\", 20064), (\"wasn't\", 18923), (\"they're\", 17813), (\"there's\", 15496), (\"You're\", 14183), (\"Let's\", 14027), (\"He's\", 11952), (\"we're\", 11627), (\"couldn't\", 11414), (\"haven't\", 11232), (\"There's\", 10682), ('.\\n.', 10393), (\"let's\", 9858), (\"what's\", 9789), (\"shouldn't\", 9764), (\"hasn't\", 8089), (\"What's\", 8013), (\"Canada's\", 7936), (\"you've\", 7662), (\"weren't\", 6301), (\"Here's\", 5995), (\"Obama's\", 5957), ('. . .', 5590), (\"They're\", 5548), (\"she's\", 5321), (\"one's\", 5301), (\"people's\", 5299), (\"you'd\", 5159), (\"we'll\", 5032), (\"they've\", 4899), (\"We're\", 4875), (\"Can't\", 4744), (\"they'll\", 4714), (\"we've\", 4673), (\"today's\", 4575), (\"Trudeau's\", 4433), (\"who's\", 4340), (\"Isn't\", 4201), (\"Alaska's\", 3913), (\"God's\", 3456), (\"he'll\", 3290), (\"ain't\", 3089), (\"women's\", 3042), (\"Didn't\", 2947), (\"Doesn't\", 2941), (\"they'd\", 2912), (\"She's\", 2909), (\"world's\", 2852), (\"America's\", 2807), (\"he'd\", 2772), (\"Clinton's\", 2750), (\"You've\", 2693), (\"We've\", 2607), (\"else's\", 2390), (\"Hillary's\", 2370), (\"we'd\", 2207), (\"gov't\", 2202), (\"We'll\", 2196), (\"country's\", 2165), ('tRump', 2128), (\"hadn't\", 2065), (\"state's\", 2030), (\"man's\", 1996), (\"government's\", 1966), (\"person's\", 1951), (\"someone's\", 1941), ('alt-right', 1939), (\"Harper's\", 1853), (\"here's\", 1793), (\"nation's\", 1740), ('.\\n\\n.', 1714), (\"You'll\", 1709), ('Brexit', 1664), (\"They've\", 1654), ('. .', 1612), (');', 1568), (\"Wouldn't\", 1562), (\"everyone's\", 1560), (\"Where's\", 1549), (\"Hawaii's\", 1532), ('):', 1528), (\"Putin's\", 1502), ('. ...', 1464), ('anti-Trump', 1436), (\"China's\", 1433), (\"it'll\", 1426), (\"woman's\", 1336), (\"You'd\", 1304), (\"Church's\", 1295), (\"Who's\", 1228), (\"They'll\", 1216), (\"i'm\", 1213)]\n",
      "Contractions have been replaced. Time since start:  0:05:54\n",
      "Emojis have been converted to text. Time since start:  0:06:14\n",
      "Smilies have been converted to text. Time since start:  0:06:20\n",
      "All signs have been removed. Time since start:  0:06:57\n",
      "All numbers have been replaced with ###. Time since start:  0:07:01\n",
      "Special symbols have been processed. Time since start:  0:07:20\n",
      "Yr and hr have been replaced by year and hour. Time since start:  0:07:22\n",
      "Correction dictionary of unknown words prepared. Time since start:  0:07:22\n",
      "Unknown words replaced excluding coverage check. Time since start:  0:13:47\n",
      "Found embeddings for 68.00% of vocab\n",
      "Found embeddings for  99.74% of all text\n",
      "Top unknown words are: [('Brexit', 1740), ('theglobeandmail', 1343), ('Drumpf', 1186), ('deplorables', 1026), ('SB91', 778), ('theguardian', 735), ('bigly', 481), ('Klastri', 452), ('Auwe', 387), ('2gTbpns', 381), ('Vinis', 322), ('Saullie', 298), ('shibai', 293), ('Koncerned', 288), ('SJWs', 281), ('TFWs', 279), ('civilbeat', 272), ('RangerMC', 271), ('klastri', 252), ('BCLibs', 248), ('Trudope', 247), ('garycrum', 246), ('Daesh', 241), ('wiliki', 232), ('gofundme', 226), ('Donkel', 224), ('OBAMAcare', 222), ('cashapp', 221), ('Finicum', 220), ('Cheetolini', 216), ('11e7', 211), ('Beyak', 210), ('Trudeaus', 210), ('Tridentinus', 204), ('dailycaller', 203), ('Ontariowe', 202), ('washingtontimes', 200), ('Zupta', 196), ('YUGE', 195), ('bavius', 188), ('11e6', 186), ('Nageak', 186), ('Meggsy', 185), ('MUPTE', 184), ('Nurnie', 179), ('Kealohas', 175), ('Crapwell', 169), ('MAGAphants', 167), ('l2g', 167), ('Deplorables', 166), ('neening', 165), ('stunents', 165), ('SHOPO', 161), ('motleycrew', 161), ('financialpost', 157), ('Twitler', 154), ('clickbait', 153), ('ncronline', 149), ('hodad', 149), ('pgtype', 149), ('scientificamerican', 149), ('TheDonald', 148), ('talkingpointsmemo', 146), ('907AK', 145), ('Mahawker', 144), ('FakeNews', 142), ('khadr', 141), ('diverdave', 141), ('Wiliki', 139), ('Catou', 138), ('flexpipe', 136), ('KABATA', 136), ('Dejain', 136), ('Krookwell', 135), ('LesterP', 134), ('commondreams', 134), ('Lazeelink', 133), ('akleg', 133), ('djou', 131), ('Pizzagate', 130), ('gubmit', 130), ('antifluoridationists', 130), ('Bozievich', 130), ('Moyane', 129), ('Exedus', 129), ('staradvertiser', 129), ('whataboutism', 127), ('Dotard', 126), ('Ossoff', 126), ('Clodwell', 125), ('covfefe', 122), ('RadirD', 120), ('ITMFA', 119), ('skyofblue', 119), ('gubmut', 117), ('brexit', 116), ('hkrieger', 116), ('Sarasi', 115), ('22moneybay', 115), ('thedailybeast', 115)]\n",
      "Pre-processing done including coverage check. Time since start:  0:19:44\n"
     ]
    }
   ],
   "source": [
    "print('\\n Glove \\n')\n",
    "df = preprocessing_NN(train_data,model_vocab_glove, calc_coverage=True, print_oov_num=100)  #if you set calc_coverage=False it reaches 14 min for whole train_set,-  however it does not show the coverage after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "colab_type": "code",
    "id": "RU-Zq8Eo6A9-",
    "outputId": "bd5c9802-f4cb-456d-d4a8-5f4a1970ba76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is so cool . It's like , ' would you want your mother to read this ? ? ' Really great idea , well done !\n",
      "Thank you ! ! This would make my life a lot less anxiety-inducing . Keep it up , and don't let anyone get in your way !\n",
      "This is such an urgent design problem ; kudos to you for taking it on . Very impressive !\n",
      "Is this something I'll be able to install on my site ? When will you be releasing it ?\n",
      "haha you guys are a bunch of losers .\n",
      "ur a sh * tty comment .\n",
      "haha suck it .\n",
      "FU\n",
      "The ranchers seem motivated by mostly by greed ; no one should have the right to allow their animals destroy public land .\n",
      "It was a great show . Not a combo I'd of expected to be good together but it was .\n",
      "Wow , that sounds great .\n",
      "This is a great story . Man . I wonder if the person who yelled \" shut the fuck up ! \" at him ever heard it .\n",
      "This seems like a step in the right direction .\n",
      "It's ridiculous that these guys are being called \" protesters \" . Being armed is a threat of violence , which makes them terrorists .\n",
      "This story gets more ridiculous by the hour ! And , I love that people are sending these guys dildos in the mail now . But … if they really think there is a happy ending in this for any of them , I think they are even more deluded than all of the jokes about them assume .\n",
      "I agree ; I don't want to grant them the legitimacy of protestors . They are greedy , small-minded people who somehow seem to share the mass delusion that this is not only a good idea for themselves as individuals , but is the right thing to do for ranchers at large . Basically : take something that currently belongs to everyone , and give it to a select group of people , so they can profit .\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(df.comment_text):\n",
    "  print(x)\n",
    "  if i==15:\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W96lEtVPOR6k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cogOyIX20hXF"
   ],
   "name": "Preprocessing_final.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
