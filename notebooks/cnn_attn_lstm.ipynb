{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "401bab62-7bd9-41dc-8593-12261a95e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, timm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa47714b-536e-412c-8269-5f202f5b1a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dim   = 2048\n",
    "text_dim    = 256\n",
    "decoder_dim = 512\n",
    "attention_dim = 256\n",
    "max_length = 256\n",
    "vocab_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcc76cb-df2a-4ac2-bca2-f5d6a460f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_project = nn.Linear(image_dim, attention_dim)\n",
    "        self.decoder_project = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.attention = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "        self.gate    = nn.Linear(decoder_dim, image_dim)  # linear layer to create a sigmoid-activated gate\n",
    "\n",
    "\n",
    "    def forward(self, image_embed, decoder_hidden):\n",
    "        batch_size, num_pixel, c  = image_embed.shape\n",
    "\n",
    "        x1 = self.encoder_project(image_embed   )               # (batch_size, num_pixel, attention_dim)\n",
    "        x2 = self.decoder_project(decoder_hidden).unsqueeze(1)  # (batch_size, 1, attention_dim)\n",
    "        x = x1 + x2\n",
    "\n",
    "        a = self.attention(F.relu(x))   # (batch_size, num_pixel, attention_dim)\n",
    "        weight = self.softmax(a)\n",
    "        weighted_image_embed = (image_embed * weight).sum(dim=1)  # (batch_size, image_dim)\n",
    "\n",
    "        gate = torch.sigmoid(self.gate(decoder_hidden))\n",
    "        weighted_image_embed = gate * weighted_image_embed\n",
    "\n",
    "        weight = weight.reshape(batch_size, num_pixel)\n",
    "        return weighted_image_embed, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c139487d-74d1-42f8-bf9a-00226914b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def init_hidden_state(self, image_embed):\n",
    "        m = image_embed.mean(dim=1)\n",
    "        h = self.init_h(m)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(m)\n",
    "        return h, c\n",
    "\n",
    "    def __init__(self,):\n",
    "        super(Net, self).__init__()\n",
    "        self.encoder = nn.Sequential(*list(timm.create_model('resnet26d', pretrained=True).children())[:-2])\n",
    "\n",
    "        #---\n",
    "        self.init_h = nn.Linear(image_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(image_dim, decoder_dim)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, text_dim)\n",
    "        self.logit = nn.Linear(decoder_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.rnn = nn.LSTMCell(image_dim + text_dim, decoder_dim, bias=True)\n",
    "\n",
    "        #----\n",
    "        # initialization\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.logit.bias.data.fill_(0)\n",
    "        self.logit.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "\n",
    "    def forward(self, image, token, length):\n",
    "\n",
    "        image_embed = self.encoder(image)\n",
    "        batch_size,c,h,w  = image_embed.shape\n",
    "        num_pixel = w*h\n",
    "\n",
    "        image_embed = image_embed.permute(0, 2, 3, 1).contiguous()\n",
    "        image_embed = image_embed.reshape(batch_size,num_pixel, image_dim)\n",
    "\n",
    "        text_embed = self.embed(token)\n",
    "        h, c = self.init_hidden_state(image_embed)\n",
    "\n",
    "        decode_length = [l-1 for l in length]\n",
    "        max_decode_length = max(decode_length)\n",
    "        logit  = torch.zeros(batch_size, max_length, vocab_size).to(image_embed.device)\n",
    "        weight = torch.zeros(batch_size, max_length, num_pixel).to(image_embed.device)\n",
    "\n",
    "        for t in range(max_decode_length):\n",
    "            B = sum([l > t for l in decode_length])\n",
    "\n",
    "            weighted_image_embed, w = self.attention(image_embed[:B], h[:B])\n",
    "\n",
    "            h, c = self.rnn(\n",
    "                torch.cat([text_embed[:B, t, :], weighted_image_embed], dim=1),\n",
    "                (h[:B], c[:B])\n",
    "            )  # (B, decoder_dim)\n",
    "\n",
    "            l = self.logit(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            logit [:B, t, :] = l\n",
    "            weight[:B, t, :] = w\n",
    "\n",
    "            #<todo> forced teacher training?\n",
    "            zz=0\n",
    "        return logit\n",
    "\n",
    "\n",
    "\n",
    "    def forward_beam_search_decode(self, image):\n",
    "        #<todo> : beam_search decode\n",
    "        return 0\n",
    "\n",
    "    #@torch.jit.export()\n",
    "    def forward_argmax_decode(self, image):\n",
    "        batch_size = len(image)\n",
    "        device = image.device\n",
    "\n",
    "        image_embed = self.encoder(image)\n",
    "        image_embed = image_embed.permute(0, 2, 3, 1).contiguous()\n",
    "        image_embed = image_embed.reshape(batch_size, num_pixel, image_dim)\n",
    "\n",
    "        # start token for LSTM input\n",
    "        token = torch.full((batch_size,), fill_value=STOI['<sos>'], dtype=torch.long, device=device)\n",
    "        h, c = self.init_hidden_state(image_embed)  # (batch_size, decoder_dim)\n",
    "\n",
    "        #-----\n",
    "        eos = torch.LongTensor([STOI['<eos>']]).to(device)\n",
    "        pad = torch.LongTensor([STOI['<pad>']]).to(device)\n",
    "\n",
    "        probability = torch.zeros(batch_size, max_length, vocab_size, device=device)\n",
    "        predict = torch.full((batch_size, max_length), fill_value=STOI['<pad>'], dtype=torch.long, device=device)\n",
    "        for t in range(max_length):\n",
    "            text_embed = self.embed(token)\n",
    "            weighted_image_embed, w = self.attention(image_embed, h)\n",
    "\n",
    "            h, c = self.rnn(torch.cat([text_embed, weighted_image_embed], dim=1), (h, c))\n",
    "            l = self.logit(h)\n",
    "            p = F.softmax(l,-1)  # (1, vocab_size)\n",
    "            k = torch.argmax(l, -1) #predict max\n",
    "\n",
    "            probability[:, t, :] = p\n",
    "            predict[:, t] = k\n",
    "            token = k #next token\n",
    "            if ((k == eos) | (k == pad)).all():  break\n",
    "\n",
    "        return predict, probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24801ac5-dbfb-49c1-b352-61dc22de49e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_cross_entropy_loss(logit, token, length):\n",
    "    truth = token[:, 1:]\n",
    "    L = [l - 1 for l in length]\n",
    "    logit = pack_padded_sequence(logit, L, batch_first=True).data\n",
    "    truth = pack_padded_sequence(truth, L, batch_first=True).data\n",
    "    loss = F.cross_entropy(logit, truth, ignore_index=2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0476678-0695-4742-8b33-f6e8dd0a1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_check_net():\n",
    "    batch_size = 7\n",
    "    C,H,W = 3, 224, 224\n",
    "    image = torch.randn((batch_size,C,H,W))\n",
    "\n",
    "    token  = np.full((batch_size, max_length), 2, np.int64) #token\n",
    "    length = np.random.randint(5,max_length-2, batch_size)\n",
    "    length = np.sort(length)[::-1].copy()\n",
    "    for b in range(batch_size):\n",
    "        l = length[b]\n",
    "        t = np.random.choice(vocab_size,l)\n",
    "        t = np.insert(t,0,     0)\n",
    "        t = np.insert(t,len(t),1)\n",
    "        L = len(t)\n",
    "        token[b,:L]=t\n",
    "\n",
    "    token  = torch.from_numpy(token).long()\n",
    "\n",
    "\n",
    "\n",
    "    #---\n",
    "    net = Net()\n",
    "    net.train()\n",
    "\n",
    "    logit = net(image, token, length)\n",
    "    loss = seq_cross_entropy_loss(logit, token, length)\n",
    "    print('vocab_size',vocab_size)\n",
    "    print('max_length',max_length)\n",
    "    print('')\n",
    "    print(length)\n",
    "    print(length.shape)\n",
    "    print(token.shape)\n",
    "    print(image.shape)\n",
    "    print(loss)\n",
    "    print('---')\n",
    "\n",
    "    print(logit.shape)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8129ccc-f089-4b6d-947b-6b87098953a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 50\n",
      "max_length 256\n",
      "\n",
      "[240 237 178 166 106  75  11]\n",
      "(7,)\n",
      "torch.Size([7, 256])\n",
      "torch.Size([7, 3, 224, 224])\n",
      "tensor(3.9208, grad_fn=<NllLossBackward>)\n",
      "---\n",
      "torch.Size([7, 256, 50])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "run_check_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b0c78-80c0-470a-ad2c-ce295e828239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
