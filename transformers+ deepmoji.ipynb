{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bert-config/vocab.txt\n",
      "/kaggle/input/bert-config/bert_config_large.json\n",
      "/kaggle/input/bert-config/bert_config.json\n",
      "/kaggle/input/tensorflow2-question-answering/sample_submission.csv\n",
      "/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\n",
      "/kaggle/input/tensorflow2-question-answering/simplified-nq-train.jsonl\n",
      "/kaggle/input/transformers/transformers-master/requirements.txt\n",
      "/kaggle/input/transformers/transformers-master/MANIFEST.in\n",
      "/kaggle/input/transformers/transformers-master/.gitignore\n",
      "/kaggle/input/transformers/transformers-master/LICENSE\n",
      "/kaggle/input/transformers/transformers-master/CONTRIBUTING.md\n",
      "/kaggle/input/transformers/transformers-master/hubconf.py\n",
      "/kaggle/input/transformers/transformers-master/.coveragerc\n",
      "/kaggle/input/transformers/transformers-master/requirements-dev.txt\n",
      "/kaggle/input/transformers/transformers-master/README.md\n",
      "/kaggle/input/transformers/transformers-master/setup.py\n",
      "/kaggle/input/transformers/transformers-master/docs/requirements.txt\n",
      "/kaggle/input/transformers/transformers-master/docs/README.md\n",
      "/kaggle/input/transformers/transformers-master/docs/Makefile\n",
      "/kaggle/input/transformers/transformers-master/docs/source/index.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/bertology.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/examples.md\n",
      "/kaggle/input/transformers/transformers-master/docs/source/migration.md\n",
      "/kaggle/input/transformers/transformers-master/docs/source/benchmarks.md\n",
      "/kaggle/input/transformers/transformers-master/docs/source/installation.md\n",
      "/kaggle/input/transformers/transformers-master/docs/source/multilingual.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/pretrained_models.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/notebooks.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/conf.py\n",
      "/kaggle/input/transformers/transformers-master/docs/source/converting_tensorflow_models.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/torchscript.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/serialization.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/quickstart.md\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/xlnet.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/transformerxl.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/gpt.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/gpt2.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/roberta.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/xlm.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/distilbert.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/ctrl.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/auto.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/bert.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/main_classes/processors.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/main_classes/configuration.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/main_classes/model.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/main_classes/optimizer_schedules.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/main_classes/tokenizer.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/imgs/warmup_constant_schedule.png\n",
      "/kaggle/input/transformers/transformers-master/docs/source/imgs/warmup_cosine_schedule.png\n",
      "/kaggle/input/transformers/transformers-master/docs/source/imgs/warmup_cosine_warm_restarts_schedule.png\n",
      "/kaggle/input/transformers/transformers-master/docs/source/imgs/warmup_cosine_hard_restarts_schedule.png\n",
      "/kaggle/input/transformers/transformers-master/docs/source/imgs/warmup_linear_schedule.png\n",
      "/kaggle/input/transformers/transformers-master/docs/source/imgs/transformers_logo_name.png\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/css/huggingface.css\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/css/Calibre-Light.ttf\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/css/Calibre-Medium.otf\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/css/code-snippets.css\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/css/Calibre-Thin.otf\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/css/Calibre-Regular.otf\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/js/custom.js\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/js/huggingface_logo.svg\n",
      "/kaggle/input/transformers/transformers-master/docker/Dockerfile\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_example_script/utils_xxx.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_example_script/run_xxx.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_example_script/README.md\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/modeling_tf_xxx.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/tokenization_xxx.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/modeling_xxx.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/README.md\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/configuration_xxx.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/tests/tokenization_xxx_test.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/tests/modeling_tf_xxx_test.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/tests/modeling_xxx_test.py\n",
      "/kaggle/input/transformers/transformers-master/.circleci/config.yml\n",
      "/kaggle/input/transformers/transformers-master/.circleci/deploy.sh\n",
      "/kaggle/input/transformers/transformers-master/examples/requirements.txt\n",
      "/kaggle/input/transformers/transformers-master/examples/run_generation.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_glue.py\n",
      "/kaggle/input/transformers/transformers-master/examples/utils_multiple_choice.py\n",
      "/kaggle/input/transformers/transformers-master/examples/utils_ner.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_bertology.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_squad.py\n",
      "/kaggle/input/transformers/transformers-master/examples/utils_summarization.py\n",
      "/kaggle/input/transformers/transformers-master/examples/utils_squad_evaluate.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_multiple_choice.py\n",
      "/kaggle/input/transformers/transformers-master/examples/utils_squad.py\n",
      "/kaggle/input/transformers/transformers-master/examples/utils_summarization_test.py\n",
      "/kaggle/input/transformers/transformers-master/examples/README.md\n",
      "/kaggle/input/transformers/transformers-master/examples/test_examples.py\n",
      "/kaggle/input/transformers/transformers-master/examples/benchmarks.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_summarization_finetuning.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_ner.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_tf_glue.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_lm_finetuning.py\n",
      "/kaggle/input/transformers/transformers-master/examples/contrib/run_openai_gpt.py\n",
      "/kaggle/input/transformers/transformers-master/examples/contrib/README.md\n",
      "/kaggle/input/transformers/transformers-master/examples/contrib/run_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers-master/examples/contrib/run_swag.py\n",
      "/kaggle/input/transformers/transformers-master/examples/contrib/run_camembert.py\n",
      "/kaggle/input/transformers/transformers-master/examples/tests_samples/.gitignore\n",
      "/kaggle/input/transformers/transformers-master/examples/tests_samples/MRPC/train.tsv\n",
      "/kaggle/input/transformers/transformers-master/examples/tests_samples/MRPC/dev.tsv\n",
      "/kaggle/input/transformers/transformers-master/examples/tests_samples/SQUAD/dev-v2.0-small.json\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/requirements.txt\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/grouped_batch_sampler.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/lm_seqs_dataset.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/distiller.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/train.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/README.md\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/run_squad_w_distillation.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/utils.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/scripts/extract_distilbert.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/scripts/token_counts.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/scripts/extract.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/scripts/binarized_data.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/training_configs/distilbert-base-uncased.json\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/training_configs/distilgpt2.json\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_bert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_bert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_encoder_decoder.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_auto.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_transfo_xl_utilities.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_auto.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_auto.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_distilbert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_gpt2.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_openai.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_roberta.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_distilbert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_gpt2.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_xlm.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_xlnet.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_roberta.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_transfo_xl_utilities.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_camembert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_xlm.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_xlm.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_ctrl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_distilbert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_ctrl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_gpt2.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_distilbert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_camembert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_bert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_roberta.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_xlnet.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_openai.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_beam_search.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_roberta.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/__main__.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_openai.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_xlm.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_auto.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_ctrl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_pytorch_checkpoint_to_tf2.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/optimization.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_xlnet.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_pytorch_utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_openai.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/__init__.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_gpt2.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_ctrl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_camembert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_bert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_xlnet.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/file_utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/data/__init__.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/data/processors/glue.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/data/processors/__init__.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/data/processors/utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/data/metrics/__init__.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_transfo_xl_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_auto_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_openai_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_tests_commons.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_transfo_xl_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_roberta_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_xlnet_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_gpt2_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_distilbert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_utils_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_auto_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_common_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_ctrl_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_ctrl_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_xlm_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/optimization_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_xlm_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_bert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/conftest.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_roberta_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_xlnet_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_gpt2_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_openai_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_bert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_roberta_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_bert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/__init__.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_encoder_decoder_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_gpt2_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_ctrl_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_auto_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_xlnet_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_distilbert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_distilbert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_common_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_openai_gpt_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_transfo_xl_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_xlm_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/configuration_common_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/fixtures/input.txt\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/fixtures/test_sentencepiece.model\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/fixtures/sample_text.txt\n",
      "/kaggle/input/transformers/transformers-master/notebooks/Comparing-TF-and-PT-models-SQuAD.ipynb\n",
      "/kaggle/input/transformers/transformers-master/notebooks/Comparing-PT-and-TF-models.ipynb\n",
      "/kaggle/input/transformers/transformers-master/notebooks/Comparing-TF-and-PT-models.ipynb\n",
      "/kaggle/input/transformers/transformers-master/notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb\n",
      "/kaggle/input/transformers/transformers-master/.github/stale.yml\n",
      "/kaggle/input/transformers/transformers-master/.github/ISSUE_TEMPLATE/migration.md\n",
      "/kaggle/input/transformers/transformers-master/.github/ISSUE_TEMPLATE/---new-benchmark.md\n",
      "/kaggle/input/transformers/transformers-master/.github/ISSUE_TEMPLATE/--new-model-addition.md\n",
      "/kaggle/input/transformers/transformers-master/.github/ISSUE_TEMPLATE/feature-request.md\n",
      "/kaggle/input/transformers/transformers-master/.github/ISSUE_TEMPLATE/question-help.md\n",
      "/kaggle/input/transformers/transformers-master/.github/ISSUE_TEMPLATE/bug-report.md\n",
      "/kaggle/input/sacremoses/sacremoses-master/.appveyor.yml\n",
      "/kaggle/input/sacremoses/sacremoses-master/requirements.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/.travis.yml\n",
      "/kaggle/input/sacremoses/sacremoses-master/CONTRIBUTORS.md\n",
      "/kaggle/input/sacremoses/sacremoses-master/README.md\n",
      "/kaggle/input/sacremoses/sacremoses-master/setup.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/subwords.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/tokenize.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/cli.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/normalize.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/chinese.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/__init__.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/truecase.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/util.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/corpus.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.it\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.yue\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.pt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.de\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.lv\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.pl\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ro\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/README.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.es\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.sl\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.fi\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.en\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.el\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.zh\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.cs\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.hu\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.is\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ga\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.sk\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ca\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.nl\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ta\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ru\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.sv\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.fr\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.lt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Symbol.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsPi.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsAlnum-unichars-au.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Hiragana.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Katakana.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Open_Punctuation.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/CJKSymbols.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsUpper.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Close_Punctuation.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsAlpha.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsN.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Hangul.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsPf.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Line_Separator.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/CJK.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsLower.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsSo.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsAlnum.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Number.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsSc.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Lowercase_Letter.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Hangul_Syllables.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsAlpha-unichars-au.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Punctuation.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Separator.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Uppercase_Letter.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Titlecase_Letter.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Currency_Symbol.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Han.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/test/test_truecaser.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/test/test_corpus.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/test/test_normalizer.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/test/test_tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/sacremoses/sacremoses-master\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses==0.0.35) (1.13.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses==0.0.35) (7.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses==0.0.35) (0.14.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from sacremoses==0.0.35) (4.39.0)\r\n",
      "Building wheels for collected packages: sacremoses\r\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=882724 sha256=dc5e2e5bd08fc6743a5168f04879afa0bc3deada5305ccbf8bf3409575cabd37\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/82/48/4b/05cb49d913a40c9d76f97931cd747d72fb17a77b0f6415cdba\r\n",
      "Successfully built sacremoses\r\n",
      "Installing collected packages: sacremoses\r\n",
      "Successfully installed sacremoses-0.0.35\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/sacremoses/sacremoses-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/transformers/transformers-master\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers==2.1.1) (1.17.4)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers==2.1.1) (1.10.29)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==2.1.1) (2.22.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from transformers==2.1.1) (4.39.0)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from transformers==2.1.1) (2019.11.1)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from transformers==2.1.1) (0.1.83)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==2.1.1) (0.0.35)\r\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.29 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.1.1) (1.13.29)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.1.1) (0.9.4)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.1.1) (0.2.1)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.1.1) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.1.1) (2019.9.11)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.1.1) (1.24.2)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.1.1) (3.0.4)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.1.1) (1.13.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.1.1) (7.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.1.1) (0.14.0)\r\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.29->boto3->transformers==2.1.1) (0.15.2)\r\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.29->boto3->transformers==2.1.1) (2.8.0)\r\n",
      "Building wheels for collected packages: transformers\r\n",
      "  Building wheel for transformers (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-2.1.1-cp36-none-any.whl size=334890 sha256=30a03dd1cd74e44bd4bd79699d05102c6cd14a7b86d25fcdb48696955a683c1b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/f3/1a/ee7248890cb4b8e8975988b1a67999e2d09ef54ce8ee815255\r\n",
      "Successfully built transformers\r\n",
      "Installing collected packages: transformers\r\n",
      "Successfully installed transformers-2.1.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/transformers/transformers-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE_NAME = \"../input/tensorflow2-question-answering/simplified-nq-train.jsonl\"\n",
    "TEST_FILE_NAME = \"../input/tensorflow2-question-answering/simplified-nq-test.jsonl\"\n",
    "\n",
    "VOCAB_SIZE = 30522\n",
    "TEXT_MAX_SEQUENCE_LENGTH = 500\n",
    "QUESTION_MAX_SEQUENCE_LENGTH = 25\n",
    "\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VAL_BATCH_SIZE = 64\n",
    "\n",
    "DATALOADER_NUM_WORKERS = 8\n",
    "\n",
    "TOKENIZER = BertTokenizer.from_pretrained(\"../input/bert-config/vocab.txt\")\n",
    "CONFIG = BertConfig.from_pretrained(\"../input/bert-config/bert_config.json\")\n",
    "# print(CONFIG)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "from math import floor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def print_number_of_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print(\"Number of trainable parameters in the model are : {}\".format(params))\n",
    "    return\n",
    "\n",
    "def get_results_dict(y_test, y_pred):\n",
    "    results = {\n",
    "        \"f1\": f1_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"precision\": precision_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred)\n",
    "    }\n",
    "    return results\n",
    "    \n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for question answering.\"\"\"\n",
    "    \n",
    "    def __init__(self, text, question, label, example_id, document_url):\n",
    "        self.text = text\n",
    "        self.question = question\n",
    "        self.label = label\n",
    "        self.example_id = example_id\n",
    "        self.document_url = document_url\n",
    "\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    cleantext = re.sub(\"\\s+\", \" \", cleantext)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def get_generator(file_name, read_batch_size):\n",
    "    with open(file_name, \"r\") as file_:\n",
    "        batch = []\n",
    "        \n",
    "        for idx, item in enumerate(file_):\n",
    "            if len(batch) == read_batch_size:\n",
    "                batch = []\n",
    "            \n",
    "            batch.append(json.loads(item))\n",
    "            \n",
    "            if len(batch) == read_batch_size:\n",
    "                yield batch\n",
    "                \n",
    "\n",
    "def create_input_examples_from_rec(rec, negative_sampling_percent=0.5):\n",
    "    text = rec[\"document_text\"].split()\n",
    "    question_text = rec[\"question_text\"].strip()\n",
    "    question_text = cleanhtml(\" \".join(question_text)).strip().lower()\n",
    "    document_url = rec[\"document_url\"]\n",
    "    example_id = rec[\"example_id\"]\n",
    "\n",
    "    long_answer_start_token = rec[\"annotations\"][0][\"long_answer\"][\"start_token\"]\n",
    "    long_answer_end_token = rec[\"annotations\"][0][\"long_answer\"][\"end_token\"]\n",
    "    long_answer_candidate_idx = rec[\"annotations\"][0][\"long_answer\"]['candidate_index']\n",
    "\n",
    "    long_answer_candidates = rec[\"long_answer_candidates\"]\n",
    "\n",
    "    temp_input_examples_list = []\n",
    "\n",
    "    # removing true label \n",
    "    if long_answer_start_token != -1:\n",
    "        long_answer_candidates = long_answer_candidates[: long_answer_candidate_idx] \\\n",
    "        + long_answer_candidates[long_answer_candidate_idx + 1 :]\n",
    "\n",
    "        # adding true label\n",
    "        temp_text = text[long_answer_start_token:long_answer_end_token]\n",
    "        temp_text = cleanhtml(\" \".join(temp_text)).strip().lower()\n",
    "        \n",
    "        if len(temp_text.split()) > 0 and len(question_text.split()) > 0:\n",
    "            temp_input_examples_list.append(\n",
    "                InputExample(\n",
    "                    text=temp_text, \n",
    "                    question=question_text,\n",
    "                    label=1,\n",
    "                    example_id=example_id,\n",
    "                    document_url=document_url\n",
    "                )\n",
    "            )\n",
    "\n",
    "    num_negative_samples = floor(len(long_answer_candidates) * negative_sampling_percent)\n",
    "    sampled_negative_samples = random.sample(long_answer_candidates, num_negative_samples)\n",
    "\n",
    "    # adding negative samples\n",
    "    for candidate in sampled_negative_samples:\n",
    "        candidate_start = candidate[\"start_token\"]  \n",
    "        candidate_end = candidate[\"end_token\"]\n",
    "\n",
    "        temp_text = text[candidate_start:candidate_end]\n",
    "        temp_text = cleanhtml(\" \".join(temp_text)).strip().lower()\n",
    "        if len(temp_text.split()) > 0 and len(question_text.split()) > 0:\n",
    "            temp_input_examples_list.append(\n",
    "                InputExample(\n",
    "                    text=temp_text, \n",
    "                    question=question_text,\n",
    "                    label=0,\n",
    "                    example_id=example_id,\n",
    "                    document_url=document_url\n",
    "                )\n",
    "            )\n",
    "    return temp_input_examples_list\n",
    "\n",
    "\n",
    "class InputFeature(object):\n",
    "    def __init__(\n",
    "        self, text_input_ids, ques_input_ids, \n",
    "        text_seq_length, ques_seq_length,label\n",
    "    ):\n",
    "        self.text_input_ids = text_input_ids\n",
    "        self.ques_input_ids = ques_input_ids\n",
    "        self.text_seq_length = text_seq_length\n",
    "        self.ques_seq_length = ques_seq_length\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "def convert_one_example_to_feature(\n",
    "    example, tokenizer,\n",
    "    text_max_seq_length=None,\n",
    "    ques_max_seq_length=None\n",
    "):\n",
    "    # featurinzing text\n",
    "    text_input_words = tokenizer.tokenize(example.text)\n",
    "    text_input_ids = tokenizer.convert_tokens_to_ids(text_input_words)\n",
    "    text_seq_length = len(text_input_words)\n",
    "    \n",
    "    if text_max_seq_length:\n",
    "        if text_seq_length > text_max_seq_length:\n",
    "            text_input_ids = text_input_ids[:text_max_seq_length]\n",
    "            text_seq_length = text_max_seq_length\n",
    "        else:\n",
    "            text_input_ids = text_input_ids + [tokenizer.pad_token_id]*(\n",
    "                text_max_seq_length - text_seq_length\n",
    "            )\n",
    "    \n",
    "    # featurizing question\n",
    "    ques_input_words = tokenizer.tokenize(example.question)\n",
    "    ques_input_ids = tokenizer.convert_tokens_to_ids(ques_input_words)\n",
    "    ques_seq_length = len(ques_input_words)\n",
    "    \n",
    "    if ques_max_seq_length:\n",
    "        if ques_seq_length > ques_max_seq_length:\n",
    "            ques_input_ids = ques_input_ids[:ques_max_seq_length]\n",
    "            ques_seq_length = ques_max_seq_length\n",
    "        else:\n",
    "            ques_input_ids = ques_input_ids + [tokenizer.pad_token_id]*(\n",
    "                ques_max_seq_length - ques_seq_length\n",
    "            )\n",
    "    \n",
    "    feature = InputFeature(\n",
    "        text_input_ids=text_input_ids,\n",
    "        ques_input_ids=ques_input_ids,\n",
    "        label = example.label,\n",
    "        text_seq_length=text_seq_length,\n",
    "        ques_seq_length=ques_seq_length\n",
    "    )\n",
    "    return feature\n",
    "    \n",
    "\n",
    "def load_cache_examples_multiprocessing(\n",
    "    examples, tokenizer, text_max_seq_length, ques_max_seq_length\n",
    "):\n",
    "    pool = multiprocessing.Pool()\n",
    "    features = pool.map(\n",
    "        partial(\n",
    "            convert_one_example_to_feature,\n",
    "            tokenizer=tokenizer,\n",
    "            text_max_seq_length=text_max_seq_length,\n",
    "            ques_max_seq_length=ques_max_seq_length\n",
    "        ),\n",
    "        examples\n",
    "    )\n",
    "    pool.close()\n",
    "    \n",
    "    # convert to Tensors and build dataset\n",
    "    all_text_input_ids = torch.tensor(\n",
    "        [f.text_input_ids for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_ques_input_ids = torch.tensor(\n",
    "        [f.ques_input_ids for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_text_seq_lengths = torch.tensor(\n",
    "        [f.text_seq_length for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_ques_seq_lengths = torch.tensor(\n",
    "        [f.ques_seq_length for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_labels = torch.tensor(\n",
    "        [f.label for f in features], dtype=torch.float\n",
    "    )\n",
    "    dataset = TensorDataset(\n",
    "        all_text_input_ids, all_ques_input_ids, all_text_seq_lengths,\n",
    "        all_ques_seq_lengths, all_labels\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# takes the last hidden_state as the question encoding\n",
    "\n",
    "class BiLstmEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, embedding_dim,\n",
    "        hidden_size, num_layers, output_dim,\n",
    "        bidirectional, dropout_ratio=0.2\n",
    "    ):\n",
    "        super(BiLstmEncoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim= embedding_dim\n",
    "        )\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            num_layers=num_layers,\n",
    "            hidden_size=hidden_size,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_ratio,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        if bidirectional is True: \n",
    "            self.linear_combiner = nn.Linear(num_layers*2*hidden_size, output_dim)\n",
    "        else:\n",
    "            self.linear_combiner = nn.Linear(num_layers*1*hidden_size, output_dim)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout_ratio)\n",
    "    \n",
    "    def forward(self, question, ques_seq_lengths=None):\n",
    "        # question = [batch_size, sent_length]\n",
    "\n",
    "        batch_size = question.shape[0]\n",
    "    \n",
    "        embedded = self.embedding(question)\n",
    "        # embedded = [batch_size, sent_length, embedding_dim]\n",
    "\n",
    "        _, (hidden, _) = self.lstm_layer(embedded)\n",
    "        # hidden = [num_layers * num_directions, batch_size, hidden_size]\n",
    "\n",
    "        hidden = hidden.view(batch_size, -1)\n",
    "        # hidden = [batch_size, num_layers * num_directions * hidden_size]\n",
    "\n",
    "        combined_context = self.linear_combiner(self.dropout_layer(hidden))\n",
    "        # combined_context = [batch_size, output_dim]\n",
    "\n",
    "        return combined_context\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of channels across timesteps (1 parameter pr. channel).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, attention_size, device\n",
    "    ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_size = attention_size\n",
    "        self.device = device\n",
    "\n",
    "        self.attention = nn.Parameter(torch.rand(attention_size))\n",
    "\n",
    "    def forward(self, inputs, input_lengths):\n",
    "        # inputs = [batch_size, max_seq_length, attention_size]\n",
    "        # input_lengths = [batch_size]\n",
    "\n",
    "        max_seq_length = inputs.shape[1]\n",
    "\n",
    "        attn = torch.matmul(inputs, self.attention)\n",
    "        # attn = [batch_size, maz_seq_len]\n",
    "\n",
    "        idxes = torch.arange(0, max_seq_length, out=torch.LongTensor(max_seq_length)).unsqueeze(0).to(self.device)\n",
    "        mask = torch.autograd.Variable((idxes < input_lengths.unsqueeze(1)).float()).to(self.device)\n",
    "        # mask = [batch_size, max_seq_length]\n",
    "        # idxes = [batch_size, max_seq_length]\n",
    "\n",
    "        attn_masked = attn.masked_fill(mask == 0, -1e10)\n",
    "        attention_weights = F.softmax(attn_masked, dim=1)\n",
    "        # attention_weights = [batch_size, max_seq_length]\n",
    "\n",
    "        # apply attention weights\n",
    "        weighted = torch.bmm(attention_weights.unsqueeze(1), inputs)\n",
    "        # weighted = [batch_size, 1, attention_size]\n",
    "\n",
    "        weighted = weighted.squeeze(1)\n",
    "        # weighted_outputs = [batch_size, attention_size]\n",
    "\n",
    "        return (weighted, attention_weights)\n",
    "\n",
    "\n",
    "class AttentiveBilstm(nn.Module):\n",
    "    def __init__(\n",
    "        self, max_seq_length,\n",
    "        vocab_size, embedding_dim,\n",
    "        hidden_size, num_layers, output_dim,\n",
    "        bidirectional, device, dropout_ratio=0.2\n",
    "    ):\n",
    "        super(AttentiveBilstm, self).__init__()\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.device = device\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            num_layers=num_layers,\n",
    "            hidden_size=hidden_size,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_ratio\n",
    "        )\n",
    "        \n",
    "        if bidirectional is True:\n",
    "            self.attention_layer = Attention(\n",
    "                attention_size=hidden_size*2,\n",
    "                device=device\n",
    "            )\n",
    "            self.linear_combiner = nn.Linear(\n",
    "                hidden_size*2, output_dim\n",
    "            )\n",
    "        else:\n",
    "            self.attention_layer = Attention(\n",
    "                attention_size=hidden_size*1,\n",
    "                device=device\n",
    "            )\n",
    "            self.linear_combiner = nn.Linear(\n",
    "                hidden_size, output_dim\n",
    "            ) \n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout_ratio)\n",
    "    \n",
    "    def forward(self, question, seq_lengths):\n",
    "        # question = [batch_size, max_seq_length]\n",
    "        # seq_lengths = [batch_size]\n",
    "\n",
    "        embedded = self.dropout_layer(self.embedding(question))\n",
    "        # embedded = [batch_size, max_seq_length, embedding_dim]\n",
    "        print(embedded.shape)\n",
    "\n",
    "        # permuting for pad packed easiness\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        # embedded = [max_seq_length, batch_size, embedding_dim]\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, seq_lengths, enforce_sorted=False\n",
    "        )\n",
    "    \n",
    "        packed_outputs, (_, _) = self.lstm_layer(packed_embedded)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, total_length=self.max_seq_length)\n",
    "        # outputs = [max_seq_length, batch_size, num_directions*hidden_size]\n",
    "\n",
    "        # outputs are permuted again because attention layer needs batch_first\n",
    "        (weighted_outputs, attention_weights) = self.attention_layer(outputs.permute(1, 0, 2), seq_lengths)\n",
    "        # weighted_outputs = [batch_size, attention_size]\n",
    "        \n",
    "        weighted_outputs = self.linear_combiner(weighted_outputs)\n",
    "        # weighted_outputs = [batch_size, output_dim]\n",
    "\n",
    "        return (weighted_outputs, attention_weights)\n",
    "\n",
    "\n",
    "class DeepMoji(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, embedding_dim, hidden_state_size,\n",
    "        num_layers, output_dim, device,\n",
    "        dropout_ratio=0.5, bidirectional=True\n",
    "    ):\n",
    "\n",
    "        super(DeepMoji, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.num_layers=num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.bidirectional = bidirectional\n",
    "        self.output_dim = output_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.bilstm_one = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_state_size,\n",
    "            bidirectional=bidirectional,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_ratio\n",
    "        )\n",
    "        \n",
    "        self.lstm_one_context_combiner_layer = nn.Linear(\n",
    "            2*hidden_state_size, hidden_state_size\n",
    "        )\n",
    "        \n",
    "        self.bilstm_two = nn.LSTM(\n",
    "            input_size=hidden_state_size,\n",
    "            hidden_size=hidden_state_size,\n",
    "            bidirectional=bidirectional,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_ratio\n",
    "        )\n",
    "        self.lstm_two_context_combiner_layer = nn.Linear(\n",
    "            2*hidden_state_size, hidden_state_size\n",
    "        )\n",
    "        \n",
    "        self.attn_layer = Attention(hidden_state_size*2 + embedding_dim, device)\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_state_size*2 + embedding_dim, output_dim)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout_ratio)\n",
    "    \n",
    "    def forward(self, inp, src_len):\n",
    "        # inp = [batch_size, sent_length]\n",
    "        # src_len = [batch_size]\n",
    "        \n",
    "        embedded = self.dropout_layer(self.embedding(inp)).permute(1, 0, 2)\n",
    "        # embedded = [sent_length, batch_size, embedding_dim]\n",
    "        \n",
    "        embedded_packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, src_len, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        bilstm_out_1_packed, (_, _) = self.bilstm_one(embedded_packed)\n",
    "        # bilstm_out_1 = [seq_len, batch_size, 2 * hidden_state_size] \n",
    "        \n",
    "        bilstm_out_1, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            bilstm_out_1_packed, total_length=embedded.shape[0])\n",
    "\n",
    "        bilstm_out_1_context_combined = self.lstm_one_context_combiner_layer(\n",
    "            bilstm_out_1\n",
    "        )\n",
    "        # bilstm_out_1_context_combined = [seq_len, batch_size, hidden_state_size]\n",
    "\n",
    "        bilstm_2_input_packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            bilstm_out_1_context_combined, src_len, enforce_sorted=False\n",
    "        )\n",
    "        bilstm_out_2_packed, (_, _) = self.bilstm_two(bilstm_2_input_packed) \n",
    "        # bilstm_out_2_packed = [seq_len, batch_size, 2 * hidden_state_size]\n",
    "        \n",
    "        bilstm_out_2, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            bilstm_out_2_packed, total_length=embedded.shape[0]\n",
    "        )\n",
    "        \n",
    "        bilstm_out_2_context_combined = self.lstm_two_context_combiner_layer(\n",
    "            bilstm_out_2\n",
    "        )\n",
    "        # bilstm_out_2_context_combined = [seq_len, batch_size, hidden_state_size]\n",
    "    \n",
    "        bilstm_stacked = torch.cat(\n",
    "            (\n",
    "                bilstm_out_1_context_combined,\n",
    "                bilstm_out_2_context_combined\n",
    "            ), \n",
    "            dim=2\n",
    "        )\n",
    "        # bilstm_stacked = [seq_len, batch_size, 2 * hidden_state_size]\n",
    "\n",
    "        # stacking embedded to bilstm_stacked\n",
    "        bilstm_and_embedded_stacked = torch.cat(\n",
    "            (\n",
    "                bilstm_stacked,\n",
    "                embedded\n",
    "            ), \n",
    "            dim=2\n",
    "        )    \n",
    "        # bilstm_and_embedded_stacked = [seq_len, batch_size, 2 * hidden_state_size + embedding_dim]\n",
    "        \n",
    "                \n",
    "        (weighted_outputs, attention_weights) = self.attn_layer(\n",
    "            bilstm_and_embedded_stacked.permute(1, 0, 2), src_len\n",
    "        )\n",
    "        # weighted_outputs = [batch_size, attention_size]\n",
    "\n",
    "        outputs = self.output_layer(weighted_outputs)\n",
    "        # outputs = [batch_size, output_dim]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, ques_embedding_dim,\n",
    "        text_embedding_dim,\n",
    "        ques_embedder,\n",
    "        text_embedder, device,\n",
    "        first_combiner_size=100,\n",
    "        dropout_ratio=0.2\n",
    "    ):\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        self.text_embedding_dim = text_embedding_dim\n",
    "        self.ques_embedding_dim = ques_embedding_dim\n",
    "        self.text_embedder = text_embedder\n",
    "        self.ques_embedder = ques_embedder\n",
    "        self.device = device\n",
    "\n",
    "        self.linear_combiner_1 = nn.Linear(\n",
    "            text_embedding_dim + ques_embedding_dim, first_combiner_size\n",
    "        )\n",
    "        self.linear_combiner_2 = nn.Linear(\n",
    "            first_combiner_size, 1\n",
    "        )\n",
    "        self.dropout_layer = nn.Dropout(dropout_ratio)\n",
    "    \n",
    "    def forward(\n",
    "        self, text_input_ids, ques_input_ids,\n",
    "        text_seq_lengths, ques_seq_lengths\n",
    "    ):\n",
    "        # text_input_ids = [batch_size, text_max_seq_length]\n",
    "        # ques_input_ids = [batch_size, ques_max_seq_length]\n",
    "        # text_seq_lengths = [batch_size]\n",
    "        # quest_seq_lengths = [batch_size]\n",
    "        # labels = [batch_size]\n",
    "        \n",
    "        text_encoded = self.text_embedder(text_input_ids, text_seq_lengths)\n",
    "        # text_encoded = [batch_size, text_embedding_dim]\n",
    "        \n",
    "        ques_encoded = self.ques_embedder(ques_input_ids, ques_seq_lengths)\n",
    "        # ques_encoded = [batch_size, ques_embedding_dim]\n",
    "        \n",
    "        # stacking both\n",
    "        stacked_ques_text = torch.cat(\n",
    "            (\n",
    "                ques_encoded,\n",
    "                text_encoded\n",
    "            ), \n",
    "            dim=1\n",
    "        )\n",
    "        # stacked_ques_text = [batch_size, ques_embedding_dim + text_embedding_dim]\n",
    "        \n",
    "        fc_out_1 = self.linear_combiner_1(self.dropout_layer(stacked_ques_text))\n",
    "        # fc_out_1 = [batch_size, first_combiner_size]\n",
    "        \n",
    "        output = self.linear_combiner_2(fc_out_1)\n",
    "        # output = [batch_size, 1]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_encoder = BiLstmEncoder(\n",
    "    vocab_size=VOCAB_SIZE, embedding_dim=50,\n",
    "    hidden_size=64, num_layers=1, output_dim=100,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "text_encoder = DeepMoji(\n",
    "    vocab_size=VOCAB_SIZE, embedding_dim=300, hidden_state_size=256,\n",
    "    num_layers=2, output_dim=500,\n",
    "    dropout_ratio=0.5, bidirectional=True, device=DEVICE\n",
    ")\n",
    "\n",
    "model = QAModel(\n",
    "    ques_embedding_dim=100,\n",
    "    text_embedding_dim=500,\n",
    "    ques_embedder=ques_encoder,\n",
    "    text_embedder=text_encoder,\n",
    "    first_combiner_size=200,\n",
    "    dropout_ratio=0.2, device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "criterion = criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_from_generator(\n",
    "    model, train_generator, optimizer, criterion,\n",
    "    negative_sampling_percent, tokenizer,\n",
    "    scheduler, print_stats_at_step=20,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    generator_num_workers=DATALOADER_NUM_WORKERS,\n",
    "    text_max_seq_length=TEXT_MAX_SEQUENCE_LENGTH,\n",
    "    ques_max_seq_length=QUESTION_MAX_SEQUENCE_LENGTH,\n",
    "    device=DEVICE\n",
    "):\n",
    "    tr_loss = 0.0\n",
    "    avg_tr_loss = 0.0\n",
    "    \n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    \n",
    "    model.train()\n",
    "    step = 0\n",
    "    for generator_idx, generator_batch in enumerate(train_generator):\n",
    "        all_data = []\n",
    "\n",
    "        for rec in generator_batch:\n",
    "            all_data.extend(create_input_examples_from_rec(rec, negative_sampling_percent=0.2))\n",
    "\n",
    "        generator_train_dataset = load_cache_examples_multiprocessing(\n",
    "            all_data, tokenizer, text_max_seq_length=text_max_seq_length,\n",
    "            ques_max_seq_length=ques_max_seq_length\n",
    "        )\n",
    "\n",
    "        generator_random_sampler = RandomSampler(generator_train_dataset)\n",
    "        generator_data_loader = DataLoader(\n",
    "            generator_train_dataset, sampler=generator_random_sampler,\n",
    "            batch_size=train_batch_size,\n",
    "            num_workers=generator_num_workers\n",
    "        )\n",
    "        \n",
    "        generator_batch_iterator = tqdm(generator_data_loader)\n",
    "        for batch_idx, batch_data in enumerate(generator_batch_iterator):\n",
    "            if batch_data[0].shape[0] == train_batch_size:\n",
    "                model.zero_grad()\n",
    "                \n",
    "                batch_data = tuple(t.to(device) for t in batch_data)\n",
    "                inputs = {\n",
    "                    \"text_input_ids\": batch_data[0],\n",
    "                    \"ques_input_ids\": batch_data[1],\n",
    "                    \"text_seq_lengths\": batch_data[2], \n",
    "                    \"ques_seq_lengths\": batch_data[3]\n",
    "                }\n",
    "                true_labels = batch_data[4]\n",
    "                \n",
    "                # getting outputs \n",
    "                logits = model(**inputs).squeeze(1)\n",
    "                \n",
    "                # propagating loss backwards and scheduler and optimizer steps\n",
    "                loss = criterion(logits, true_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                step_loss = loss.item()\n",
    "                \n",
    "                tr_loss += step_loss\n",
    "                avg_tr_loss += step_loss\n",
    "                \n",
    "                # for calculation of results matrix\n",
    "                if preds is None:\n",
    "                    preds = torch.round(F.sigmoid(logits)).detach().cpu().numpy()\n",
    "                    out_label_ids = true_labels.detach().cpu().numpy()\n",
    "                else:\n",
    "                    preds = np.append(\n",
    "                        preds,\n",
    "                        torch.round(F.sigmoid(logits)).detach().cpu().numpy(),\n",
    "                        axis=0\n",
    "                    )\n",
    "                    out_label_ids = np.append(\n",
    "                        out_label_ids,\n",
    "                        true_labels.detach().cpu().numpy(),\n",
    "                        axis=0\n",
    "                    )\n",
    "                if step % print_stats_at_step == 0:\n",
    "                    tr_loss = tr_loss / print_stats_at_step\n",
    "                    results = get_results_dict(out_label_ids, preds)\n",
    "                    # writing on bar\n",
    "                    generator_batch_iterator.set_description(\n",
    "                        f'Tr Iter: {step}, avg_step_loss: {tr_loss:.4f}, avg_tr_loss: {(avg_tr_loss / (step + 1)):.4f}, tr_f1: {results[\"f1\"]:.4f}, tr_prec: {results[\"precision\"]:.4f}, tr_rec: {results[\"recall\"]:.4f}, tr_acc: {results[\"accuracy\"]:.4f}'\n",
    "                    )\n",
    "                    tr_loss = 0.0\n",
    "                    preds = None\n",
    "                    out_label_ids = None\n",
    "                step += 1\n",
    "                \n",
    "        print(f\"{generator_idx + 1} generator is completed.\")\n",
    "        if generator_idx > 1:\n",
    "            break\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = get_generator(TRAIN_FILE_NAME, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tr Iter: 0, avg_step_loss: 0.0352, avg_tr_loss: 0.7044, tr_f1: 0.3254, tr_prec: 0.4875, tr_rec: 0.3371, tr_acc: 0.4688: 100%|██████████| 9/9 [00:06<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 generator is completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 generator is completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tr Iter: 20, avg_step_loss: 0.1736, avg_tr_loss: 0.1988, tr_f1: 0.4935, tr_prec: 0.4890, tr_rec: 0.4980, tr_acc: 0.9742: 100%|██████████| 10/10 [00:07<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 generator is completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_epoch_from_generator(\n",
    "    model, data_generator, optimizer=optimizer, criterion=criterion,\n",
    "    negative_sampling_percent=0.1, tokenizer=TOKENIZER,\n",
    "    scheduler=scheduler, print_stats_at_step=20,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    generator_num_workers=DATALOADER_NUM_WORKERS,\n",
    "    text_max_seq_length=TEXT_MAX_SEQUENCE_LENGTH,\n",
    "    ques_max_seq_length=QUESTION_MAX_SEQUENCE_LENGTH,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
